= Nodes
:page-aliases: clustersetup:file-locations

[abstract]
A Couchbase-Server _cluster_ consists of one or more _nodes_, each of which is a system running an instance of Couchbase Server.

[#nodes-and-their-creation]
== Nodes and their Creation
A Couchbase Server _node_ is a physical or virtual machine that hosts a single instance of Couchbase Server.
The establishment of the server on the node entails four stages:

.	_Installed_. Couchbase Server has been fully installed on the node, but not yet started.

.	_Started_. Couchbase Server has been started.
Set-up can now be performed, using Couchbase Web Console, the CLI, or the REST API.

. _Initialized_. Optionally, up to three custom paths have been specified on the current node, respectively corresponding to the locations at which the node's data, indexes, and analytics are to be saved. Note that if this stage is skipped, and initialization therefore not explicitly performed, path-setting may occur during subsequent provisioning.

. _Provisioned_. The username and password for the Full Administrator must have been specified.
Additionally, services, service memory-quotas, and buckets may have been specified.

When any variant of stage 4 has been achieved, the node is considered to be provisioned, and thereby, to be a cluster of one server; and re-initialization is not permitted.

Note that running multiple instances of Couchbase Server on a single node is not supported.

[#paths-for-data-indexes-and-analytics]
=== Paths for Data, Indexes, and Analytics

The full, default path for each supported platform is shown in the following table:

.Default paths for data-files
[cols="1,2"]
|===
| Platform | Default directory

| Linux
| [.path]_/opt/couchbase/var/lib/couchbase/data_


| Windows
| [.path]_C:\Program Files\couchbase\server\var\lib\couchbase\data_

| Mac OS X
| [.path]_~/Library/Application Support/Couchbase/var/lib/couchbase/data_
|===

Note that once it has been specified, the data-file path location should not be used to store any data other than that allocated by Couchbase Server; since all such additional data will be deleted.

The data-file path cannot be spontaneously changed on a node that is active within a cluster: therefore, you should apply the correct, permanent data-file path at initialization, prior to provisioning.
If a new path is required for a node that has already been provisioned, the node must be reinitialized; which means that all results of prior provisioning are erased.

[#clusters]
== Clusters

A Couchbase _cluster_ consists of one or more systems, each running Couchbase Server.
An existing cluster can be incremented with additional nodes.
This can be accomplished in either of the following ways:

[#node-addition]
* The routine for _adding_ a new node to the existing cluster is executed on the existing cluster.
+
An instance of Couchbase Server must have been installed on the available node, and must be at stage 2, 3, or 4: that is, must itself be started and uninitialized; or started and initialized; or started, initialized, and provisioned (which means, itself a cluster of one node).
Adding the available node means that:

** Any custom paths already established on the available node are kept unchanged on the available node.
Note that this allows each individual node within a cluster to maintain its data, indexes, and analytics in its own, node-specific location.

** If the node is at stage 4, all the results of its prior provisioning are deleted.
This includes services, memory-quotas, buckets, bucket-data, and Full Administrator username and password.

** The services and memory-quotas that are currently the default for the cluster can be optionally assigned to the node that is being added.
However, an error occurs if the node does not have sufficient memory.
Services and memory-quotas for the node can be configured to be other than the default.
Alternatively, the default itself can be changed, provided that it does not require more of a given resource than is available on every node currently in the cluster.

[#node-joining]
* The routine for _joining_ an existing cluster is executed on the new node.
+
The available node must be at stage 2 or 3: that is, it must have been started, and may have had its data, index, and analytics paths configured.
However, it cannot have been provisioned in any way: if the routine for joining is executed on a provisioned node, an error is flagged, and the operation fails.
+
Note that services can nevertheless be assigned to the new node during the join operation itself.
The
memory quota for each service defaults to the setting for the existing cluster.
An error occurs if the new node does not have sufficient memory.

Once a cluster has been created, any of the IP addresses of the cluster-nodes can be used to access data and services.
Therefore, provided that one node in the cluster is running the Data Service, the IP address of another node - one that is not running the Data Service - can be specified, in order to access the Data Service: the Cluster Manager ensures that all requests are appropriately routed across the cluster.

[#rebalance-and-fail-over]
== Rebalance, Removal, Failover, and Recovery

_Rebalance_ is a process of re-distributing data and indexes among available nodes.
This process should be run whenever a node is added to or removed from an existing cluster as part of a scheduled or pre-planned maintenance activity.
It can also be run after a node has been taken out of the cluster by means of _failover_, described below.
Rebalance takes place while the cluster is running and servicing requests: clients continue to read and write to existing structures, while the data is being moved between Data Service nodes.
Once data-movement has completed, the updated distribution is communicated to all applications and other relevant consumers.
See xref:learn:clusters-and-availability/rebalance.adoc[Rebalance], for more information.

Node _removal_ allows a node to be taken out of a cluster in a highly controlled fashion, using _rebalance_ to redistribute data and indexes among remaining nodes.
It is to be used only when are nodes in the cluster are responsive.
It can be used on any node.
See xref:learn:clusters-and-availability/removal.adoc[Removal], for more information.

_Failover_ is the process by which a cluster-node can be removed; either _proactively_, to support required maintenance, or _reactively_, in the event of an outage.
Two types of failover are supported, which are _graceful_ (for Data Service nodes only) and _hard_ (for nodes of any kind).
Both types can be applied manually when needed.
_Hard_ can also be applied automatically, by means of prior configuration: in which case it becomes known as _automatic_ failover.
See xref:learn:clusters-and-availability/failover.adoc[Failover], for more information.

_Recovery_ allows a previously failed-over node to be added back into its original cluster, by means of the _rebalance_ operation.
_Full_ recovery involves removing all pre-existing data from, and assigning new data to, the node that is being recovered.
_Delta_ recovery maintains and resynchronizes a nodeâ€™s pre-existing data.
See xref:learn:clusters-and-availability/recovery.adoc[Recovery], for more information.

[#Node Quantity]
== Node Quantity

For production purposes, clusters of less than three nodes are not recommended.
For information, see xref:install:deployment-considerations-lt-3nodes.adoc[About Deploying Clusters with Less than Three Nodes].

[#hostnames]
== Hostnames

Initial configuration of Couchbase Server allows the node to be referenced by means of a default IP address: 127.0.0.1.
You may choose to specify a different IP address, or a hostname.
The hostname must be valid, and must ultimately resolve to a valid IP Address.

If a node is restarted, Couchbase Server continues to use the specified hostname.
Note, however, that if the node is failed over or removed, Couchbase Server will no longer use the specified hostname: therefore, in such circumstances, the node must be reconfigured, and the hostname re-specified.
