= Nodes
:page-aliases: clustersetup:file-locations,install:hostnames

[abstract]
A Couchbase-Server _cluster_ consists of one or more _nodes_, each of which is a system running an instance of Couchbase Server.

[#nodes-and-their-creation]
== Nodes and their Creation
A Couchbase Server _node_ is a physical or virtual machine that hosts a single instance of Couchbase Server.
The establishment of the server on the node entails four stages:

.	_Installed_. Couchbase Server has been fully installed on the node, but not yet started.

.	_Started_. Couchbase Server has been started.
Set-up can now be performed, using Couchbase Web Console, the CLI, or the REST API.

. _Initialized_. Optionally, up to three custom paths have been specified on the current node, respectively corresponding to the locations at which the node's data, indexes, and analytics are to be saved. Note that if this stage is skipped, and initialization therefore not explicitly performed, path-setting may occur during subsequent provisioning.

. _Provisioned_. The username and password for the Full Administrator must have been specified.
Additionally, services, service memory-quotas, and buckets may have been specified.

When any variant of stage 4 has been achieved, the node is considered to be provisioned, and thereby, to be a cluster of one server; and re-initialization is not permitted.

Note that running multiple instances of Couchbase Server on a single node is not supported.

[#paths-for-data-indexes-and-analytics]
=== Paths for Data, Indexes, and Analytics

The full, default path for each supported platform is shown in the following table:

.Default paths for data-files
[cols="1,2"]
|===
| Platform | Default directory

| Linux
| [.path]_/opt/couchbase/var/lib/couchbase/data_


| Windows
| [.path]_C:\Program Files\couchbase\server\var\lib\couchbase\data_

| Mac OS X
| [.path]_~/Library/Application Support/Couchbase/var/lib/couchbase/data_
|===

Note that once it has been specified, the data-file path location should not be used to store any data other than that allocated by Couchbase Server; since all such additional data will be deleted.

The data-file path cannot be spontaneously changed on a node that is active within a cluster: therefore, you should apply the correct, permanent data-file path at initialization, prior to provisioning.
If a new path is required for a node that has already been provisioned, the node must be reinitialized; which means that all results of prior provisioning are erased.

[#clusters]
== Clusters

A Couchbase _cluster_ consists of one or more systems, each running Couchbase Server.
An existing cluster can be incremented with additional nodes.
This can be accomplished in either of the following ways:

[#node-addition]
* The routine for _adding_ a new node to the existing cluster is executed on the existing cluster.
+
An instance of Couchbase Server must have been installed on the available node, and must be at stage 2, 3, or 4: that is, must itself be started and uninitialized; or started and initialized; or started, initialized, and provisioned (which means, itself a cluster of one node).
Adding the available node means that:

** Any custom paths already established on the available node are kept unchanged on the available node.
Note that this allows each individual node within a cluster to maintain its data, indexes, and analytics in its own, node-specific location.

** If the node is at stage 4, all the results of its prior provisioning are deleted.
This includes services, memory-quotas, buckets, bucket-data, and Full Administrator username and password.

** The services and memory-quotas that are currently the default for the cluster can be optionally assigned to the node that is being added.
However, an error occurs if the node does not have sufficient memory.
Services and memory-quotas for the node can be configured to be other than the default.
Alternatively, the default itself can be changed, provided that it does not require more of a given resource than is available on every node currently in the cluster.

[#node-joining]
* The routine for _joining_ an existing cluster is executed on the new node.
+
The available node must be at stage 2 or 3: that is, it must have been started, and may have had its data, index, and analytics paths configured.
However, it cannot have been provisioned in any way: if the routine for joining is executed on a provisioned node, an error is flagged, and the operation fails.
+
Note that services can nevertheless be assigned to the new node during the join operation itself.
The
memory quota for each service defaults to the setting for the existing cluster.
An error occurs if the new node does not have sufficient memory.

Once a cluster has been created, any of the IP addresses of the cluster-nodes can be used to access data and services.
Therefore, provided that one node in the cluster is running the Data Service, the IP address of another node - one that is not running the Data Service - can be specified, in order to access the Data Service: the Cluster Manager ensures that all requests are appropriately routed across the cluster.

[#rebalance-and-fail-over]
== Rebalance, Removal, Failover, and Recovery

_Rebalance_ is a process of re-distributing data and indexes among available nodes.
This process should be run whenever a node is added to or removed from an existing cluster as part of a scheduled or pre-planned maintenance activity.
It can also be run after a node has been taken out of the cluster by means of _failover_, described below.
Rebalance takes place while the cluster is running and servicing requests: clients continue to read and write to existing structures, while the data is being moved between Data Service nodes.
Once data-movement has completed, the updated distribution is communicated to all applications and other relevant consumers.
See xref:learn:clusters-and-availability/rebalance.adoc[Rebalance], for more information.

Node _removal_ allows a node to be taken out of a cluster in a highly controlled fashion, using _rebalance_ to redistribute data and indexes among remaining nodes.
It is to be used only when are nodes in the cluster are responsive.
It can be used on any node.
See xref:learn:clusters-and-availability/removal.adoc[Removal], for more information.

_Failover_ is the process by which a cluster-node can be removed; either _proactively_, to support required maintenance, or _reactively_, in the event of an outage.
Two types of failover are supported, which are _graceful_ (for Data Service nodes only) and _hard_ (for nodes of any kind).
Both types can be applied manually when needed.
_Hard_ can also be applied automatically, by means of prior configuration: in which case it becomes known as _automatic_ failover.
See xref:learn:clusters-and-availability/failover.adoc[Failover], for more information.

_Recovery_ allows a previously failed-over node to be added back into its original cluster, by means of the _rebalance_ operation.
_Full_ recovery involves removing all pre-existing data from, and assigning new data to, the node that is being recovered.
_Delta_ recovery maintains and resynchronizes a node’s pre-existing data.
See xref:learn:clusters-and-availability/recovery.adoc[Recovery], for more information.

[#Node Quantity]
== Node Quantity

For production purposes, clusters of less than three nodes are not recommended.
For information, see xref:install:deployment-considerations-lt-3nodes.adoc[About Deploying Clusters with Less than Three Nodes].

[#hostnames]
== Hostnames

Initial configuration of Couchbase Server allows the node to be referenced by means of a default IP address: 127.0.0.1.
You may choose to specify a different IP address, or a hostname.
The hostname must be valid, and must ultimately resolve to a valid IP Address.

If a node is restarted, Couchbase Server continues to use the specified hostname.
Note, however, that if the node is failed over or removed, Couchbase Server will no longer use the specified hostname: therefore, in such circumstances, the node must be reconfigured, and the hostname re-specified.

[#node-certificates]
== Node Certificates

As described in xref:learn:security/certificates.adoc[Certificates], Couchbase Server can be protected by means of x.509 certificates; ensuring that only approved users, applications, machines, and endpoints have access to system resources; and that clients can verify the identity of Couchbase Server.

Certificate installation for a cluster places the chain certificate _chain.pem_ and the private node key _pkey.key_ in the _/opt/couchbase/var/lib/couchbase/inbox/_ folder, for each cluster-node.
If an attempt is made, either by means of _adding_ or _joining_, to incorporate a new node into the certificate-protected cluster without the new node itself already being appropriately certificate-protected, the attempt fails.
Couchbase Web Console provides the following error message, _Attention: Prepare join failed. Error applying node certificate. Unable to read certificate chain file /opt/couchbase/var/lib/couchbase/inbox/chain.pem. The file does not exist._

Therefore, a new node should be appropriately certificate-protected, before any attempt is made to incorporate it into a certificate-protected cluster.
See xref:learn:security/certificates.adoc[Certificates] for an overview of certificates in the context of Couchbase Server.
See xref:manage:manage-security/configure-server-certificates.adoc[Configure Server Certificates] for information on configuring server certificates, including certificate installation.

[#node-to-node-encryption]
== Node-to-Node Encryption

Couchbase Server supports _node-to-node encryption_, whereby network traffic between the individual nodes of a cluster is encrypted, in order to optimize cluster-internal security.
Node-to-node encryption can be established on either of two levels:

* _Control_. Cluster-management information passed between nodes by the Cluster Manager and related low-level processes is encrypted.
However, the data that is passed between nodes by Couchbase Services continues to be passed in the clear.
When node-to-node encryption is enabled, this is the default.

* _All_. Both cluster-management information and data is passed in encrypted form.
Node-to-node encryption can be supported by means of either the self-signed certificates with which nodes are protected by default; or certificates signed by a recognized Certificate Authority, and installed by the administrator.

[#managing-ip-address-families]
=== Managing IP-Address Families

Node-to-node encryption supports both the IPV4 and IPV6 address-families.
The Couchbase CLI allows selection of the family to be used for the cluster.
The following requirements apply:

* Each cluster-node must be configured with a fully qualified domain name (such as `nodename.clustername.com`).

* Each cluster-node must be operating in dual stack mode, thereby supporting both IPV4 and IPV6 addressing.

DNS records must have been updated to resolve to the appropriate addresses.

[#using-node-to-node-encryption]
=== Using Node-to-Node Encryption

Node-to-node encryption is disabled by default.
Note that auto-failover must be switched off both for a change to be made to the cluster’s address-family, and for encryption to be enabled or disabled: auto-failover can be switched back on after the necessary changes have been made.
(This requirement prevents nodes that are undergoing changes from being determined unresponsive during the process, and unnecessarily failed over.)

Node-to-node encryption is managed by means of the Couchbase CLI.
The typical command-sequence is summarized below.

. Change address family.
Both IPV4 and IPV6 addresses are supported: one or the other must be selected.
The selected setting is applied to every node in the cluster.
+
Note that in order to change the address family, auto-failover must first be disabled, since each per-node address-family change potentially results in the node appearing offline, and so getting unnecessarily failed over.
Auto-failover can be re-enabled following completion of encryption-configuration
+
The address-family is changed by means of the change-ip-family CLI command.

. Enable cluster encryption.
Cluster encryption must be specifically enabled: by default, it is disabled.
To enable or disable cluster encryption, or to determine the current setting, use the change-cluster-encryption CLI command.

. Establish cluster encryption-level.
This determines whether only cluster-management information or both cluster-management information and data will be passed in encrypted form: this are respectively the _control_ and _all_ encryption levels.
Use the setting-security CLI command to specify or to get the current setting.

. If required, re-enable auto-failover.

For a more detailed account of this sequence, see xref:manage:manage-nodes/node-to-node-encryption.adoc[Apply Node-to-Node Encryption].

[#certificate-rotation-and-node-to-node-encryption]
=== Certificate Rotation and Node-to-Node Encryption

Typically, the certificates used to protect and authenticate a cluster should be rotated.
Before this operation is performed, node-to-node encryption should be disabled; and following the operation, re-enabled, as follows:

. Use change-cluster-encryption, to disable node-to-node encryption.
Note that node-to-node encryption cannot be disabled when the encryption-level has been specified as _all_.
Before attempting to disable, determine the encryption-level, and if necessary, change it to _control_.

. Rotate certificates, using ssl-manage.

. Re-enable node-to-node encryption, using `change-cluster-encryption`, and establish the appropriate level of encryption.

[#adding-and-joining-cluster-nodes]
=== Adding and Joining Cluster-Nodes

TBA
