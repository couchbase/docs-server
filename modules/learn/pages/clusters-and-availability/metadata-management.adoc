= Metadata Management

:description: pass:q[A Couchbase Cluster's _metadata_ describes its configuration. Some categories of metadata are managed by a _consensus-based_ methodology based on the Raft algorithm; others by the _Master Services_ of the Cluster Manager. ]

[abstract]
{description}

[#understanding-couchbase-server-metadata]
== Understanding Couchbase-Server Metadata

In Couchbase Server 7.0+, metadata is considered to be of two kinds: metadata related to cluster topology; and all other metadata.
These are maintained and managed in different ways.

[#topology-related-metadata]
=== Topology-Related Metadata

Metadata related to cluster topology is maintained, with high consistency, by a consensus methodology, based on the https://raft.github.io/[Raft^] algorithm.
This includes:

* The _node-list_ for the cluster.
* The _status_ of each node.
* The _service-map_ for the cluster, which indicates on which nodes particular services have been installed.
* _Bucket definitions_, including the placement of scopes and collections.
* The _vBucket maps_ for the cluster.

The process whereby this metadata is maintained is described below, in xref:learn:clusters-and-availability/metadata-management.adoc#consensus-based-metadata-management[Consensus-Based Metadata-Management].

[#all-other-metadata]
=== All Other Metadata

All other metadata for the cluster is maintained by _ns_sever_.
This metadata includes:

* Compaction settings.
* Security settings.
* XDCR replications.
* Index definitions for GSI and Search.
* Analytics-Service shadow data-sets.
* Query-Service settings.
* Eventing-Service functions.
* Data-Service metadata.
* Backup-Service metadata.

Every node in the cluster stores a copy of all such metadata.
Metadata-changes are asynchronously replicated across the nodes by the xref:learn:clusters-and-availability/cluster-manager.adoc#master-services[Master Services] of the Cluster Manager: this leadership role is occupied by one node at a time; with a replacement node being elected if the current leader has become non-communicative.

A leader can only transmit metadata-changes when it is granted a _quorum lease_; which means that a majority of nodes in the cluster are continuing to communicate with it.
Should at least half of the cluster's nodes become non-communicative, due to an unanticipated network partition, the leader can make no further changes to metadata until the partition is healed, or the cluster is otherwise fixed.
(See xref:learn:clusters-and-availability/hard-failover.adoc#default-and-unsafe[Hard Failover in Default and Unsafe Modes], for information on failing over a cluster in such circumstances.)

The asynchronous replication of metadata-changes by the Master Services means that updates occur across the cluster with _eventual consistency_.

For a more detailed description of the cluster manager, see xref:learn:clusters-and-availability/cluster-manager.adoc[Cluster Manager].

[#consensus-based-metadata-management]
== Consensus-Based Metadata-Management

Topology data for the Couchbase Cluster &#8212; covering nodes, services, buckets, scopes and collections, and vBuckets &#8212; is managed by a consensus-based procedure that is an extension of the https://raft.github.io/[Raft^] algorithm.
Basic concepts are introduced below.

[#logs-leaders-and-followers]
=== Logs, Leaders, and Followers

Each node maintains a log, into which metadata change-directives are saved.
Across the cluster, logs are maintained with consistency.
At intervals, to control log-size and ensure availability, on each node, compaction is performed, and snapshots are taken.

Each node is considered to be in the role of either _leader_ or _follower_; with at most one node the leader at any time.
Only the leader can transmit metadata change-directives.
The role of leader may periodically be exchanged between nodes, by the process described below.

[#leadership-election]
=== Leadership and Election

The leader is responsible for communicating with clients: this includes providing the current topology of the cluster, and receiving requests for topology change.
The leader distributes clients' requests to the followers, as change-directives, which are to be appended to the log-instance on each node.

In the event of the leader becoming non-communicative (say, due to network failure), a follower can advertise itself as a _candidate_ for leadership.
The other followers, receiving the communication, respond with _votes_.
Every vote received by the candidate constitutes a follower's support for the candidacy.
If a majority of nodes are supportive, a new _term_ is started, with the elected node as leader.

Nodes are given the right to vote only when fully integrated into the cluster.
Nodes not fully integrated (as is the case, for example, during the process of their addition) are considered _replicas_.
Replicas participate in the exchange and commitment of information (see immediately below), but do not vote.
Once a node's addition is complete, the node is able to vote.

[#data-exchange-and-commitment]
=== Data Exchange and Commitment

When the leader transmits change-directives to followers, each follower appends the information to its own instance of the log.
Once a majority of nodes confirm that they have done so, the leader _commits_ the information, and informs the other nodes.
Once informed, the other nodes also commit the information.

_Committing_ means writing to the _Replicated State Machine_; which is described below.

Following commitment, the leader returns the execution-result to the client.

[#network-failures-and-consequent-inconsistencies]
=== Network Failures and Consequent Inconsistencies

When network failures prevent change-directives from reaching a majority of nodes, the information is appended to the logs of those nodes that have received it; but is not committed.
Whenever a leader commits a new log entry, it also commits all preceding entries in its log; and this commitment is applied, across the cluster, to all instances of the _Replicated State Machine_.

[#replicated-state-machine]
=== Replicated State Machine

The _Replicated State Machine_ is a key-value store, resident on each node, that contains the cluster's topographical data.
Clients that require such data receive it from the leader's key-value store.
The key-value pairs in the store are generated and updated based on change-directives appended to the replicated log.
A change-directive may:

* Add a key-value pair.
* Update the value of a key-value pair.
* Update the value of a key-value pair, based on logical constraints.
* Update the values of key-value pairs transactionally.

Each key-value pair has a revision number that corresponds to the sequence of change-directives applied to the store.

[#quorum-failure]
=== Quorum Failure

A _quorum failure_ occurs when half or more of the nodes in the cluster cannot be contacted.
In this situation, commitment to the replicated log is prohibited; until either the communication problem is remedied, or the non-communicative nodes are failed over.
In consequence, prior to remedy or failover, for the duration of the quorum failure:

- Buckets, scopes, and collections can neither be created nor dropped.

- Nodes cannot be added, joined, failed over, or removed.

See xref:learn:clusters-and-availability/hard-failover.adoc#performing-an-unsafe-failover[Performing an Unsafe Failover], for information on failing over nodes in response to a quorum failure.
