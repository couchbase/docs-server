= Server Group Awareness
:page-aliases: understanding-couchbase:clusters-and-availability/groups

[abstract]
Individual server-nodes can be assigned to specific _groups_, within a
Couchbase Cluster.
This allows _active_ vBuckets to be maintained on a different group from that
of their corresponding _replica_ vBuckets; so that if a group goes offline,
bucket-data remains available on another group.

[#understanding-server-group-awareness]
== Understanding Server Group Awareness

_Server Group Awareness_ provides enhanced availability.
Specifically, it protects a cluster from large-scale infrastructure failure, through the definition of _groups_.
Each group is created by an appropriately authorized administrator, and specified to contain a subset of the nodes within a Couchbase Cluster.
Following group-definition and rebalance, the active vBuckets for any defined bucket are located on one group, while the corresponding replicas are located on another group.
This allows _Group Failover_ to be enabled, so that if an entire group goes offline, its replica vBuckets, which remain available on another group, can be automatically promoted to active status.

Groups should be defined in accordance with the physical distribution of cluster-nodes.
For example, a group should only include the nodes that are in a single _server rack_, or in the case of cloud deployments, a single _availability zone_.
Thus, if the server rack or availability zone becomes unavailable due to a power or network failure, Group Failover, if enabled, allows continued access to the affected data.

Data-protection is optimal when groups are assigned _equal numbers of nodes_, and vBuckets are therefore distributed such that none ever occupies the same group as its associated active vBucket.
By contrast, when groups are _not_ assigned equal numbers of nodes, rebalance can only produce a _best effort_ redistribution of replica vBuckets: this may result in replica vBuckets occupying the same group as their associated active vBuckets; meaning that data may be lost if such a group becomes unavailable.

Server Group Awareness is only available in Couchbase Server Enterprise Edition.

*General notes about Server Group Awareness:*

* Server Group Awareness only applies to the Data Service.
* If a cluster contains only one node, or only one group of nodes, enabling Group Failover has no effect; meaning that failover can only be performed on a per-node basis.
* Failover should be enabled for server groups only if three or more server groups have been established, and sufficient capacity exists to absorb the load of any failed-over group.
* When you initialize a new Couchbase Server cluster, the first node, and all subsequent nodes, are automatically placed in a server group named Group 1.
Once you create additional server groups, you are required to specify a server group when adding additional cluster nodes.

For information on failing over individual nodes, see
xref:manage:manage-nodes/fail-nodes-over.adoc[Fail a Node over and Rebalance].

For information on vBuckets, see xref:buckets-memory-and-storage/buckets.adoc[Buckets].

For information on the standard (non-Group-based) distribution of replica vBuckets across a cluster, see xref:clusters-and-availability/replication-architecture.adoc[Availability].

[#vbucket-distribution-across-equal-groups]
== Equal Groups

The following illustration shows how vBuckets are distributed across two groups; each group containing four of its cluster's eight nodes.

[#groups_two_equal]
image::clusters-and-availability/groups-two-equal.png[,720,align=left]

Note that Group 2 contains all the replica vBuckets that correspond to active vBuckets on Group 1; while conversely, Group 1 contains all the replica vBuckets that correspond to active vBuckets on Group 2.

[#vbucket-distribution-across-unequal-groups]
== Unequal Groups

The following illustration shows how vBuckets are distributed across two groups: Group 1 contains four nodes, while Group 2 contains five.

[#groups_two_unequal]
image::clusters-and-availability/groups-two-unequal.png[,720,align=left]

Group 1 contains all the replica vBuckets that correspond to active vBuckets on Group 2.
However, since the groups contain unequal number of nodes, Group 2 not only contains all the replica vBuckets that correspond to active vBuckets on Group 1, but also contains all the replica vBuckets for its own additional node, Server 9 â€” the replicas for Server 9 being distributed across the other Group 2 nodes; which are Servers 5, 6, 7, and 8.
Server 9 contains its own active vBuckets, plus replica vBuckets for Group 1.

This means that if Group 2 were to go offline, _Group Failover_ would not preserve the replica vBuckets for Server 9, since these only existed on Group 2 itself.

[#node-failover-across-groups]
== Node-Failover Across Groups

When an individual node within a group goes offline, rebalance provides a _best effort_ redistribution of replica vBuckets.
This keeps all data available, but results in some data being no longer protected by the Groups mechanism.
This is shown by the following illustration, in which Server 2, in Group 1, has gone offline, and a rebalance and failover have occurred.

[#groups_two_failover_one_node]
image::clusters-and-availability/groups-two-failover-one-node.png[,720,align=left]

With the active vBuckets on Server 2 no longer accessible, the replica vBuckets for Server 2 have been promoted to active status, on the servers of Group 2.
The data originally active on Server 2 is thereby kept available.
Note, however, that if Group 2 were now to go offline, the data originally active on Server 2 would be lost, since it now exists only on Group 2 servers.

[#adding-multiple-groups]
== Adding Multiple Groups

When multiple groups are to be added to a cluster simultaneously, the additions should all be executed on a _single node_ of the cluster: this simplifies the reconfiguration process, and so protects against error.

[#group-failover-and-service-availability]
== Group Failover and Service Availability

When groups are defined to correspond to racks or availability zones, other services required for data access &#8212; such as the Index Service and the Search Service &#8212; should be deployed so as to ensure their own continued availability, during the outage of a rack or zone.

For example, given a cluster:

* Whose Data Service deployment supports two Server Groups, each corresponding to one of two racks

* Whose data must be continuously accessed by the Index and Search Services

At a minimum, one instance of the Index Service and one instance of the Search Service should be deployed on each rack; and all indexes should be similarly replicated.

[#defining-groups-and-enabling-group-failover]
== Defining Groups and Enabling Group Failover

To define and manage groups:

* With Couchbase Web Console, see xref:manage:manage-groups/manage-groups.adoc[Manage Groups].
* With CLI, see xref:cli:cbcli/couchbase-cli-group-manage.adoc[group-manage].
* With the REST API, see xref:rest-api:rest-rza.adoc[Server Groups API].

To enable Group Failover:

* With Couchbase Web Console, see the information provided for the *General* settings panel, in xref:manage:manage-settings/general-settings.adoc#node-availability[Node Availability].
* With CLI, see xref:cli:cbcli/couchbase-cli-setting-autofailover.adoc[setting-autofailover].
* With the REST API, see xref:rest-api:rest-cluster-autofailover-enable.adoc[Enabling and Disabling Auto-Failover].
