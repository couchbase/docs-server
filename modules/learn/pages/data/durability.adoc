= Durability

[abstract]
_Durability_ ensures the greatest likelihood of data-writes surviving unexpected anomalies, such as node-outages.

[#understanding-durability]
== Understanding Durability

Clients writing to Couchbase Server can optionally specify _durability requirements_; which instruct Couchbase Server to update the specified document on multiple nodes in memory and/or disk locations across the cluster; before considering the write to be committed.
The greater the number of memory and/or disk locations specified in the requirements, the greater the level of _durability_ achieved.

Once the write has been committed as specified by the requirements, Couchbase Server notifies the client of success.
If commitment was not possible, Couchbase Server notifies the client of failure; and the data retains its former value throughout the cluster.

Couchbase Server supports durability for single-document writes only.
This form of write is referred to as a _durable_ or _synchronous_ write.

[#durability-benefits-and-costs]
== Benefits and Costs

Durability allows developers to distinguish between regular and durable writes.

* A _regular_ write is _asynchronous_, and is not supported by durability.
Such a write may be appropriate when writing only a small amount of data, the loss of which would produce only a minor inconvenience.

* A _durable_ write is _synchronous_, and is supported by durability.
Such a write may be appropriate when saving a large amount of data, the loss of which could have a considerable, negative impact on the user, and possibly on the system as a whole.

Note that in most cases, a durable write takes significantly longer than a regular write; due to the increased amount of either replication or persistence it demands from Couchbase Server, prior to completion.
Application-performance might therefore suffer, if durable writes are used to excess.

[#majority]
== Majority

Client-specified durability requirements use the concept of _majority_, to indicate the number of configured Data Service nodes to which commitment is required, based on the number of replicas defined for the bucket.
The correspondences are as follows:

[cols="2,2"]
|===
| Number of Replicas | Number of Nodes Required for Majority

| 0
| 1

| 1
| 2

| 2
| 2

| 3
| 3
|===

[#durability-requirements]
== Durability Requirements

The durability requirements specified by a client are:

* _Level_.
The level of durability required.
The possible values are:

** _majority_.
The mutation must be replicated to (that is, held in the bucket's memory on) a majority of the Data Service nodes.

** _majorityAndPersistActive_.
The mutation must be replicated to a majority of the Data Service nodes.
Additionally, it must be _persisted_ (that is, written and synchronised to disk) on the node hosting the active vBucket for the data.

** _persistToMajority_.
The mutation must be persisted to a majority of the Data Service nodes.
Accordingly, it will be written to disk on those nodes.

* _Timeout_.
The number of milliseconds within which the durability requirements must be met.
If not specified, a server-defined default is used.
If the number is exceeded, the entire synchronous write is aborted.

[#process-and-communication]
== Process and Communication

A durable write is atomic.
Client-reads of the value undergoing a durable write return the value that precedes the durable write.
Clients that attempt to write to a key whose value is undergoing a durable write receive an error message from the active node, and may retry.

[#durable-write-lifecycle]
The lifecycle of a durable write is shown by the following, annotated diagram; which shows multiple clients' attempts to update and read a value, with the timeline moving from left to right.

image::data/durabilityDiagram.png[,780,align=left]

The annotations are as follows:

. Client 1 specifies durability requirements for a durable write, to change a key’s existing value, of a, to b.

. The Active Node receives the request, and the durable write process is initiated.
During the course of the durable write, Couchbase Server attempts to meet the client’s specified durability requirements.

. During the durable write process, Client 2 performs a read on the value undergoing the durable write.
Couchbase Server returns the value, a,  that preceded the durable-write request.

. During the durable-write process, Client 3 attempts either a durable write or an regular write on the value that is already undergoing durable write.
Couchbase Server returns an error.
(See below for information on regular writes.)

. At the point the mutation has met the specified durability requirements, the Active Node commits the durable write, and sends a status response of SUCCESS to Client 1.

. After the durable-write process, Client 2 performs a second read on the  value.
Couchbase Server returns the value, b, committed by the durable write.
Indeed, from this point, all clients see the value b.
If Couchbase Server aborts a durable write, all mutations to active and replica vBuckets in memory and on disk are rolled back, and all copies of the data are reverted to their value from before the data-write.
Couchbase Server duly informs the client with an error message.
See xref:learn:data/durability.adoc#failure-scenarios[Failure Scenarios], below.

In some circumstances, rather than acknowledging to a client that the durable write has been either committed or aborted, Couchbase Server acknowledges an ambiguous outcome: for example, due to the client-specified timeout having elapsed.

A client that specifies durability requirements should not disconnect from the server before receiving acknowledgement of the durable write’s success or failure.

Subsequent to a durable write’s commitment and due acknowledgement, Couchbase Server continues the process of replication and persistence, until all active and replica vBuckets, both in memory and on disk, have been appropriately updated across all nodes.

[#regular-writes]
== Regular Writes

If a client writes data to Couchbase Server without specifying durability requirements, this is considered a _regular_ (that is _asynchronous_) write.
By default, no durability requirement is imposed.
The client receives acknowledgment of a successful write as soon as the data is in memory, on the node hosting the active vBucket.
A regular write provides no guarantee of durability.

[#failure-scenarios]
== Failure Scenarios

A durable write fails in the following situations:

. _Server timeout exceeded_.
The active node aborts the durable write, issues an undo request to all replica nodes, and informs the client that the durable write has had an ambiguous result.

. _Replica node fails while SyncWrite is pending (that is, before the active node can identify whether the node hosted a replica)_.
If enough alternative replica nodes can be identified, the durable write can proceed.
Otherwise, the active node waits until a server-side timeout has expired; then aborts the durable write, and duly informs the client.

. _Active node fails while SyncWrite is pending_.
This disconnects the client, which must assume that the result of the durable write has proved ambiguous.
If the active node is failed over, a replica is promoted from a replica node: depending on how advanced the durable write was at the time of active-node failure, the durable write may proceed.

. _Write while SyncWrite is pending_.
A client that attempts a durable or an asynchronous write on a key whose value is currently undergoing a durable write receives a corresponding error message.
The client may retry.

[#rebalance]
== Rebalance

The _rebalance_ process moves active and replica vBuckets across nodes, to ensure optimal availability.
During the process, clients’ access to data is uninterrupted.
The durable-write process is likewise uninterrupted by rebalance, and continues throughout the rebalance process.

[#protection-guarantees-overview]
== Protection Guarantees: Overview

When the durable-write process is complete, the application is notified that _commitment_ has occurred.
During the time-period that starts at the point of commitment, and lasts until the point at which the new data has been fully propagated throughout the cluster (this being potentially but not necessarily later than the point of commitment), if an outage occurs, the new data is guaranteed to be protected from loss &#8212; _within certain constraints_.
The constraints are related to the _level_ of durability specified by the client, the nature of the outage, and the number of replicas.
The guarantees and associated constraints are stated on this page, below.

[#protection-guarantees-and-automatic-failover]
=== Protection Guarantees and Automatic Failover

xref:learn:clusters-and-availability/automatic-failover.adoc[Automatic Failover] removes a non-responsive node from the cluster automatically, following an administrator-configured timeout.
Active vBuckets thereby lost are replaced by the promotion of replica vBuckets, on the surviving nodes.
A maximum of three sequential automatic failovers can be configured to occur.

In cases where persistence-based commitment has occurred without (as yet) propagation of the new data to the remaining nodes, if outages render non-communicative the nodes that contain the new data, failover of those nodes results in the data's loss &#8212; since no updated replica vBuckets yet exist elsewhere on the cluster.

To prevent this, and thereby maintain guaranteed protection, at least one of the non-communicative nodes containing the new data should _not_ be failed over; and auto-failover should be configured to occur sequentially only up to the number of times that supports this requirement.

[#protection-guarantees-1-replica]
== Protection Guarantees: One Replica

When one replica has been defined, from the point of commitment until the new data has been fully propagated across the cluster, protection guarantees are as follows:

[cols="2,3,5"]
|===
| *Level* | *Failure(s)* | *Description*
| _majority_
| The active node fails, and is automatically failed over.
| The new data is lost from the memory of the active node; but exists in the memory of the replica node.
The replica vBucket is promoted to active status on the replica node, and the new data is thus preserved.
|===

[cols="2,3,5"]
|===
| _majorityAndPersistActive_
| The active node fails, and is automatically failed over.
| The new data is lost from the memory of the active node; but exists in the memory of the replica node.
The replica vBucket is promoted to active status on the replica node, and the new data is thus preserved.

|
| The active node fails, but restarts before autofailover occurs.
| The new data is lost from the memory of the active node; but exists on the disk of the active node, and is thereby recovered when the active node has restarted.
|===

[cols="2,3,5"]
|===
| _persistToMajority_
| The active node fails, and is automatically failover over.
| The new data is lost from the memory and disk of the active node; but exists in the memory and disk of the replica node.
The replica vBucket is promoted to active status on the replica node, and the new data is thus preserved.

|
| The active node fails, but restarts before autofailover occurs.
| The new data is lost from the memory of the active node; but exists on the disk of the active node, and is thereby recovered when the active node has restarted.

|
| The active node fails, and is automatically failed over.
Then, the promoted replica node itself fails, and then restarts.
| The new data is lost from the memory and disk of the active node, but exists in the memory and on the disk of the replica node; and is promoted there to active status.
Then, the promoted replica node itself fails, and the new data is temporarily unavailable. However, when the promoted replica node has restarted, the new data again becomes available on disk.

To ensure auto-failover does not conflict with guaranteed protection, when two replicas have been configured, establish `1` as the maximum number of sequential automatic failovers that can take place without administrator intervention.

|===

[#protection-guarantees-2-replicas]
== Protection Guarantees: Two Replicas

The durability protection guarantees for two replicas are identical to those described above, for xref:learn:data/durability.adoc#protection-guarantees-1-replica[One Replica].
This is because _majority_ is `2` for both cases: see the table in xref:learn:data/durability.adoc#majority[Majority], above.
Commitment therefore occurs without the second replica being guaranteed an update.

To ensure auto-failover does not conflict with guaranteed protection, when two replicas have been configured, establish `1` as the maximum number of sequential automatic failovers that can take place without administrator intervention.

[#protection-guarantees-3-replicas]
== Protection Guarantees: Three Replicas

When three replicas have been defined, protection guarantees, from the point of commitment until the new data has been fully propagated across the cluster, are as described below.

Note that auto-failover should be set to occur no more than `2` times, sequentially.

[cols="2,3,5"]
|===
| *Level* | *Failure(s)* | *Description*
| _majority_
| The active node fails, and is automatically failed over.
| The new data is lost from the memory of the active node; but exists in the memory of two replica nodes.
A replica vBucket is promoted to active status on one of the replicas node, and the new data is thus preserved.

|
| The active node fails, and is automatically failed over.
Then, the promoted replica node itself fails, and is automatically failed over.
| The new data is lost from the memory of the active node; but exists in the memory of the two replica nodes.
A replica vBucket is promoted to active status on one of the replica nodes, and the new data is thus preserved.

Then, the promoted replica node itself fails, and is automatically failed over.
The new data is lost from the memory of the promoted replica node; but exists in the memory of the second replica node.
The second replica node is promoted to active status, and the new data is thus preserved.
|===

[cols="2,3,5"]
|===
| _majorityAndPersistActive_
| The active node fails, and is automatically failed over.
| The new data is lost from the memory and disk of the active node; but exists in the memory of two replica nodes.
A replica vBucket is promoted to active status on one of the replica nodes, and the new data is thus preserved.

|
| The active node fails, and is automatically failed over.
Then, the promoted replica node itself fails, and is automatically failed over.
| The new data is lost from the memory and disk of the active node; but exists in the memory of two replica nodes.
A replica vBucket is promoted to active status on one of the replica nodes, and the new data is thus preserved.

Then, the promoted replica node itself fails, and is automatically failed over.
The second replica node is promoted to active status, and the new data is thus preserved.

|
| The active node fails, but restarts before auto-failover occurs.
| The new data is lost from the memory of the active node; but exists on the disk of the active node, and is thereby recovered when the active node has restarted.

|===

[cols="2,3,5"]
|===
| _persistToMajority_
| The active node fails, and is automatically failed over.
| The new data is lost from the memory and disk of the active node; but exists in the memory and on the disks of two replica nodes.
A replica vBucket is promoted to active status on one of the replica nodes, and the new data is thus preserved.

|
| The active node fails, and is automatically failed over.
Then, the promoted replica node itself fails, and is automatically failed over.
| The new data is lost from the memory and disk of the active node; but exists in the memory and on the disks of two replica nodes.
A replica vBucket is promoted to active status on one of the replica nodes, and the new data is thus preserved.

Then, the promoted replica node itself fails, and is automatically failed over.
The second replica node is promoted to active status, and the new data is thus preserved.

|
| The active node fails, but restarts before auto-failover occurs.
| The new data is lost from the memory of the active node; but exists on the disk of the active node, and is thereby recovered when the active node has restarted.

|
| The active node fails, and is automatically failed over.
Then, the promoted replica node itself fails, and is restarted.
| The new data is lost from the memory and disk of the active node; but exists in the memory and on the disks of two replica nodes.
A replica vBucket is promoted to active status on one of the replica nodes, and the new data is thus preserved.

Then, the promoted replica node itself fails; but is restarted before the end of time-period that must elapse before auto-failover is triggered.
The new data is retrieved from disk, and is thus preserved.

|
| The active node fails, and is automatically failed over.
Then, the promoted replica node itself fails, and is automatically failed over.
The second replica node is promoted.
The second replica node itself then fails, but is restarted.
| The new data is lost from the memory and disk of the active node; but exists in the memory and on the disks of two replica nodes.
A replica vBucket is promoted to active status on one of the replica nodes, and the new data is thus preserved.

Then, the promoted replica node itself fails, and is automatically failed over.
The new data is lost from the memory and disk of the promoted replica node; but exists in the memory and on the disk of the second replica node.
A replica vBucket is promoted to active status on the second replica node, and the new data is thus preserved.

The second replica node itself then fails, but is restarted.
The new data is retrieved from disk, and is thus preserved.
|===
