= Default Token Filters
:page-topic-type: reference

The Search Service's token filters work with xref:guides:search/customize-index.adoc#tokenizers[tokenizers] to filter search input tokens. 

Use a token filter to filter a tokenizer's results and get better search result matches in your index. 

For more information about token filters, see xref:guides:search/customize-index.adoc#token-filters[Token Filters].

The following token filters are available: 

|====
|Token Filter Type |Description 

|apostrophe | Removes all characters after an apostrophe (') in a token. Also removes the apostrophe.

|camelCase a| Splits text in camelCase inside a token into separate tokens. 

For example, the token filter splits the token `camelCaseText` into `camel`, `Case`, and `Text`.

|cjk_bigram | 

|cjk_width |

|elision_ca | Removes characters that prefix a term with an apostrophe for the Catalan language. 

|elision_fr a| 

Removes characters that prefix a term with an apostrophe for the French language. 

For example, the token filter converts the token `l'avion` to `avion`.

|elision_ga | Removes characters that prefix a term with an apostrophe for the Gaelic language.

|elision_it | Removes characters that prefix a term with an apostrophe for the Italian language.

|hr_suffix_transformation_filter |

|lemmatizer_he | Lemmatizes similar forms of Hebrew words. Corrects spelling mistakes 

|mark_he | Marks the Hebrew, non-Hebrew, and numeric tokens from tokenizer results. 

|niqqud_he | 

|normalize_ar | Uses http://unicode.org/reports/tr15/[Unicode Normalization^] to normalize Arabic characters in tokens. 

|normalize_ckb | Uses http://unicode.org/reports/tr15/[Unicode Normalization^] to normalize Kurdish characters in tokens.

|normalize_de | Uses http://unicode.org/reports/tr15/[Unicode Normalization^] to normalize German characters in tokens.

|normalize_fa | Uses http://unicode.org/reports/tr15/[Unicode Normalization^] to normalize Persian characters in tokens.

|normalize_hi | Uses http://unicode.org/reports/tr15/[Unicode Normalization^] to normalize Hindi characters in tokens.

|normalize_in | Uses http://unicode.org/reports/tr15/[Unicode Normalization^] to normalize Indonesian characters in tokens.

|possessive_en | 

|reverse | Reverses the tokens from the tokenizer results. For example, the token filter converts the token `acrobat` to `taborca`.

|stemmer_ar | 

|stemmer_ckb |

|stemmer_da_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Danish language tokens into word stems. 

|stemmer_de_light |

|stemmer_de_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert German language tokens into word stems. 

|stemmer_en_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert English language tokens into word stems. 

|stemmer_es_light |

|stemmer_es_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Castilian Spanish language tokens into word stems. 

|stemmer_fi_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Finnish language tokens into word stems. 

|stemmer_fr_light | 

|stemmer_fr_min |

|stemmer_fr_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert French language tokens into word stems. 

|stemmer_hi |

|stemmer_hr |

|stemmer_hu_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Hungarian language tokens into word stems. 

|stemmer_it_light |

|stemmer_it_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Italian language tokens into word stems. 

|stemmer_nl_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Dutch language tokens into word stems. 

|stemmer_no_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Norwegian language tokens into word stems. 

|stemmer_porter | Transforms the tokens from the tokenizer results with the porter stemming algorithm. For more information, see the https://tartarus.org/martin/PorterStemmer/[official Porter Stemming Algorithm documentation^].

|stemmer_pt_light |

|stemmer_ro_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Romanian language tokens into word stems. 

|stemmer_ru_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Russian language tokens into word stems. 

|stemmer_sv_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Swedish language tokens into word stems. 

|stemmer_tr_snowball | Uses the https://snowballstem.org/[Snowball string processing language^] to convert Turkish language tokens into word stems. 

|stop_ar | Removes tokens from the tokenizer results that are unnecessary for a search, based on an Arabic dictionary. 

|stop_bg | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Bulgarian dictionary. 

|stop_ca | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Catalan dictionary. 

|stop_ckb | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Kurdish dictionary. 

|stop_cs | Removes tokens from the tokenizer results that are unnecessary for a search, based on a ? dictionary. 

|stop_da | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Danish dictionary. 

|stop_de | Removes tokens from the tokenizer results that are unnecessary for a search, based on a German dictionary. 

|stop_el | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Greek dictionary.

|[[stop-en]]stop_en | Removes tokens from the tokenizer results that are unnecessary for a search, based on an English dictionary. For example, the token filter removes `and`, `is`, and `the` from tokenizer results.

|stop_es | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Castilian Spanish dictionary.

|stop_eu | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Basque dictionary.

|stop_fa | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Persian dictionary.

|stop_fi | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Finnish dictionary.

|stop_fr | Removes tokens from the tokenizer results that are unnecessary for a search, based on a French dictionary.

|stop_ga | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Gaelic dictionary.

|stop_gl | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Galician Spanish dictionary.

|stop_he | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Hebrew dictionary.

|stop_hi | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Hindi dictionary.

|stop_hr | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Croatian dictionary.

|stop_hu | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Hungarian dictionary.

|stop_hy | Removes tokens from the tokenizer results that are unnecessary for a search, based on an Armenian dictionary.

|stop_id | Removes tokens from the tokenizer results that are unnecessary for a search, based on an Indonesian dictionary.

|stop_it | Removes tokens from the tokenizer results that are unnecessary for a search, based on an Italian dictionary.

|stop_nl | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Dutch dictionary.

|stop_no | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Norwegian dictionary.

|stop_pt | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Portuguese dictionary.

|stop_ro | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Romanian dictionary.

|stop_ru | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Russian dictionary.

|stop_sv | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Swedish dictionary.

|stop_tr | Removes tokens from the tokenizer results that are unnecessary for a search, based on a Turkish dictionary.

|[[to-lower]]to_lower | Converts all characters in tokens to lowercase. 

|unique | Removes any tokens that aren't unique. 

|====