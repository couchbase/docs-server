You can create any of the following custom token filters: 

* <<dict-compound,dict_compound>>: Use a wordlist to find and create tokens from compound words in existing tokens.
* <<edge-ngram,edge_ngram>>: Use a set character length to create tokens from the start or end of existing tokens.
* <<elision,elision>>: Use a wordlist to remove elisions from input tokens.
* <<keyword-marker,keyword_marker>>: Use a wordlist of keywords to find and create new tokens.
* <<length,length>>: Use a set character length to filter out tokens that are too long or too short.
* <<ngram,ngram>>: Use a set character length to create new tokens.
* <<normalize-unicode,normalize_unicode>>: Use Unicode Normalization to convert tokens.
* <<shingle,shingle>>: Use a set character length and separator to concatenate and create new tokens.
* <<stop-tokens,stop_tokens>>: Use a wordlist to find and remove words from tokens.
* <<truncate-token,truncate_token>>: Use a set character length to truncate existing tokens.