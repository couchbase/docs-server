= Token Filters

_Token Filters_ accept a token-stream provided by a tokenizer and make modifications to the tokens in the stream.

A frequently used form of token filtering is _stemming_; this reduces words to a base form that typically consists of the initial _stem_ of the word (for example, `play`, which is the stem of `player`, `playing`, `playable`, and more).

With the stem used as the token, a wider variety of matches can be made (for example, the input-text `player` can be matched with the document-content `playable`).

The following kinds of token-filtering are supported by Couchbase Full Text Search:

* *apostrophe*: Removes all characters after an apostrophe and the apostrophe itself. For example, `they've` becomes `they`.
* *camelCase*: Splits camelCase text to tokens.
* *dict_compound*: Allows user-specification of a dictionary whose words can be combined into compound forms, and individually indexed.
* *edge_ngram*: From each token, computes https://en.wikipedia.org/wiki/N-gram[n-grams^] that are rooted either at the front or the back.
* *elision*: Identifies and removes characters that prefix a term and are separated from it by an apostrophe.
For example, in French, `l'avion` becomes `avion`.
* *keyword_marker*: Identifies keywords and marks them as such.
These are then ignored by any downstream stemmer.
* *length*: Removes tokens that are too short or too long for the stream.
* *to_lower*: Converts all characters to lower case.
* *ngram*: From each token, computes https://en.wikipedia.org/wiki/N-gram[n-grams^].
There are two parameters, which are the minimum and maximum n-gram length.
* *reverse*: Simply reverses each token.
* *shingle*: Computes multi-token shingles from the token stream.
For example, the token stream `the quick brown fox`, when configured with a shingle minimum and a shingle maximum length of 2, produces the tokens `the quick`, `quick brown`, and `brown fox`.
* *stemmer_porter*: Transforms the token stream as per the https://tartarus.org/martin/PorterStemmer/[porter stemming algorithm^].
* *stemmer_snowball*: Uses http://snowball.tartarus.org/[libstemmer^] to reduce tokens to word-stems.
* *stop_tokens*: Removes from the stream tokens considered unnecessary for a Full Text Search. For example, `and`, `is`, and `the`. For example, `HTML` becomes `html`.
* *truncate*: Truncates each token to a maximum-permissible token-length.
* *normalize_unicode*: Converts tokens into http://unicode.org/reports/tr15/[Unicode Normalization Form^].
* *unique*: Only indexes unique tokens during analysis.

Note that the token filters are frequently configured according to the special characteristics of individual languages.
Couchbase Full Text Search provides multiple language-specific versions of the *elision*, *normalize*, *stemmer*, *possessive*, and *stop* token filters.
Specially supported languages are shown in the table immediately below.

.Supported Token-Filter Languages
[[token_filter_languages_5.5]]
[cols="1,4"]
|===
| Name | Language

| ar
| Arabic

| bg
| Bulgarian

| ca
| Catalan

| cjk
| Chinese {vbar} Japanese {vbar} Korean

| ckb
| Kurdish

| da
| Danish

| de
| German

| el
| Greek

| en
| English

| es
| Spanish (Castilian)

| eu
| Basque

| fa
| Persian

| fi
| Finnish

| fr
| French

| ga
| Gaelic

| gl
| Spanish (Galician)

| hi
| Hindi

| hu
| Hungarian

| hy
| Armenian

| id, in
| Indonesian

| it
| Italian

| nl
| Dutch

| no
| Norwegian

| pt
| Portuguese

| ro
| Romanian

| ru
| Russian

| sv
| Swedish

| tr
| Turkish
|===

