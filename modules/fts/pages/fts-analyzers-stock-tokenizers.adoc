= Tokenizers

Tokenizers split input-strings into individual tokens: characters likely to prohibit certain kinds of matching (for example, spaces or commas) are omitted.
The tokens so created are then made into a _token stream_ for the query.

The following tokenizers are available from the Couchbase Web Console:

* *Letter*: Creates tokens by breaking input-text into subsets that consist of letters only: characters such as punctuation-marks and numbers are omitted.
Creation of a token ends whenever a non-letter character is encountered.
For example, the text `Reqmnt: 7-element phrase` would return the following tokens: `Reqmnt`, `element`, and `phrase`.
* *Single*: Creates a single token from the entirety of the input-text.
For example, the text `in each place` would return the following token: `in each place`.
Note that this may be useful for handling URLs or email-addresses, which can thus be prevented from being broken at punctuation or special-character boundaries.
It may also be used to prevent multi-word phrases (for example, placenames such as `Milton Keynes` or `San Francisco`) from being broken up due to whitespace; so that they become indexed as a single term.
* *Unicode*: Creates tokens by performing _Unicode Text Segmentation_ on word-boundaries, using the https://github.com/blevesearch/segment[segment^] library.
For examples, see http://www.unicode.org/reports/tr29/#Word_Boundaries[Unicode Word Boundaries^].
* *Web*: Creates tokens by identifying and removing html tags.
For example, the text `<h1>Introduction<\h1>` would return the token `Introduction`.
* *Whitespace*: Creates tokens by breaking input-text into subsets according to where whitespace occurs.
For example, the text `in each place` would return the following tokens: `in`, `each`, and `place`.