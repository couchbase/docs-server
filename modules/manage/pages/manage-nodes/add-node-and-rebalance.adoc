= Add a Node and Rebalance
:page-aliases: clustersetup:adding-nodes

[abstract]
A new Couchbase Server node can be added to an existing cluster.

[#understanding-node-addition]
== Understanding Node-Addition

_Full_ and _Cluster_ administrators can use the UI, CLI, or REST API to add Couchbase Server nodes to existing clusters.
On each node to be added, Couchbase Server must have been _installed and started_.
The process of node-addition grants to the new node the settings already established for the parent cluster.
(See xref:manage:manage-settings/manage-settings.adoc[Manage Settings] for details.)
The process allow services to be assigned to the new node.
If the new node was previously _initialized_ with custom disk-paths, these are retained.
All aspects of _provisioning_ that may previously have occurred on the new node are eliminated: this includes buckets, services, and settings.

Following node-addition, _rebalance_ is required, to make the new node and active member of the cluster.

[#examples-on-this-page-node-addition]
== Examples on This Page

The examples in the subsections below show how to add the same node to the same one-node cluster; using the xref:manage:manage-nodes/add-node-and-rebalance.adoc#add-a-node-with-the-ui[UI], the xref:manage:manage-nodes/add-node-and-rebalance.adoc#add-a-node-with-the-cli[CLI], and the xref:manage:manage-nodes/add-node-and-rebalance.adoc#add-a-node-with-the-rest-api[REST API] respectively.
The examples assume:

* A one-node cluster already exists; and is named after its IP address: `10.142.181.101`.
It is running the Data, Query, and Index services, and has the `travel-sample` bucket installed.
(To access and install this, see xref:manage:manage-settings/install-sample-buckets.adoc[Sample Buckets].)

* A new node has been started.
This is named after its IP address: `10.142.181.101`.
It has not been initialized or provisioned.

* The cluster has the Full Administrator username of `Administrator`, and password of `password`.

[#add-a-node-with-the-ui]
== Add a Node with the UI

Proceed as follows:

. Bring up Couchbase Web Console, and log into cluster `10.142.181.101`, using the Full Administrator username and password.
Access the *Servers* tab, in the left-hand navigation bar:
+
[#left_click_on_servers_tab]
image::manage-nodes/accessServersTab.png[,90,align=middle]
+
The *Servers* screen for the cluster is now displayed:
+
[#servers-screen-initial]
image::manage-nodes/singleNodeClusterBeforeAddNode.png[,800,align=middle]
+
This shows the name of the sole node in the cluster, `10.142.181.101`, plus additional information, including the services hosted.

. Ensure that the node to be added has been started.
This can be accomplished by checking the IP address and port number for the new node in the address bar of the browser.
The following interface is displayed:
+
[#new-node-welcome-screen]
image::manage-nodes/newNodeWelcomeScreen.png[,400,align=middle]
+
This indicates that Couchbase Server is installed and running on the new node, but has not yet been provisioned.
Do not use this interface: instead, return to Couchbase Web Console for the cluster, `10.142.181.101`.

. In the *Servers* panel for the cluster, left-click on the *ADD SERVER* button, at the upper right:
+
[#add-server-button]
image::manage-nodes/addServerButton.png[,140,align=middle]
+
The *Add Server Node* dialog is now displayed:
+
[#add-server-node-dialog]
image::manage-nodes/addServerNodeDialog.png[,400,align=middle]
+
Note the warning provided at the top of the dialog: if the node to be added has already been provisioned, the results of such provisioning will be eliminated and replaced on the node's addition to the current cluster.
(In fact, the node to be added in this example, has neither been initialized nor provisioned.)

. Specify the IP address of the node to be added.
There is no need to specify a password, since the node has not yet been provisioned with one.
Uncheck all of the *Services* check-boxes except *Data*.
The dialog now appears as follows:
+
[#add-server-node-dialog-complete]
image::manage-nodes/addServerNodeDialogComplete.png[,400,align=middle]
+
Left-click on the *Add Server* button to save.
The *Servers* screen is redisplayed, with the following appearance:
+
[#servers-screen-with-node-added]
image::manage-nodes/twoNodeClusterAfterAddNodeExpanded.png[,800,align=middle]
+
This indicates that the new node, `10.142.181.102` has been successfully added.
However, it is not yet taking traffic, and will be added following a _rebalance_. Note, at this point, the figure under the *Items* column for for `10.142.181.101`: this is `31.1 K/0`, which indicates that the node contains 3.1 K items in _active_ vBuckets, and 0 items in _replica_ vBuckets.
Meanwhile, the *Items* figure for `10.142.181.102` is 0/0, indicating that no items are yet distributed onto that node in either active or replica form.
+
To access information on buckets, vBuckets, and intra-cluster replication, see the xref:learn:architecture-overview.adoc[Architecture Overview].

. To perform a rebalance, left-click on the *Rebalance* button, at the upper right:
+
[#rebalance-button]
image::manage-nodes/rebalanceButton.png[,140,align=middle]
+
[#rebalance-progress-add-node]
The new node is rebalanced into the cluster, meaning that whatever active and replica vBuckets were previously distributed across the original cluster nodes are redistributed across the superset of nodes created by the addition.
Each row shows its own ongoing workload-status.
Additionally, a *Rebalance* dialog is displayed:
+
image::manage-nodes/rebalanceInOfNodeTwo6.5.png[,800,align=middle]
+
The dialog indicates rebalance progress for each of the services on the cluster.
To see more information on the progress related to the Data Service, left-click on the *Data* tab:
+
image::manage-nodes/rebalanceOpenDataTab.png[,90,align=middle]
+
The pane expands to provide additional information on the progress of data-transfer:
+
image::manage-nodes/rebalanceOpenedDataTab.png[,430,align=middle]
+
The dialog provides the average times required in the move of each vBucket.
A set of averages is provided for each bucket on the cluster.
For an explanation of the _stages_ &#8212; *Move*, *Backfill*, *Takeover*, and *Persistence* &#8212; see xref:learn:clusters-and-availability/rebalance.adoc#data-service-rebalance-stages[Data-Service Rebalance Stages].
Note that the dialog may also include progress information on data-compression.
+
When the rebalance is complete,
*Servers* screen now appears as follows:
+
[#servers-screen-with-node-added-after-rebalance]
image::manage-nodes/twoNodeClusterAfterRebalance.png[,800,align=middle]
+
This indicates that cluster `10.142.181.101` now contains two fully functioning nodes, which are `10.142.181.101` and `10.142.181.102`.
(Note that the figure in the *Items* column for node `10.142.181.101` is `15.2 K/15.8 K`, which indicates that 15.2 K items are stored on the node in _active_ vBuckets, and 15.8 K in _replica_ vBuckets.
The figure for `10.142.181.102` indicates the converse.
Therefore, replication has successfully distributed the contents of `travel-sample` across both nodes, providing a single replica vBucket for each active vBucket.)

[#rebalance-failure-notification]
=== Rebalance Failure Notification

If rebalance fails &#8212; for example, due to a node's becoming non-responsive &#8212; Couchbase Web Console displays a notification such as the following:

image::manage-nodes/rebalanceFailureNotification.png[,250,align=middle]

As this indicates, detailed information can be found by left-clicking on the *Logs* tab, in the left-hand, vertical navigation bar.
This brings up the *Logs* screen, containing information such as the following:

image::manage-nodes/rebalanceFailureLog.png[,800,align=middle]

Information is also provided on the *Rebalance* dialog:

image::manage-nodes/rebalanceDialogFailureNotification.png[,400,align=middle]

If an unresponsive node become responsive again, rebalance can simply be reattempted manually.
Alternatively, the handling of a rebalance-failure can be configured to occur automatically, as described immediately xref:manage:manage-nodes/add-node-and-rebalance.adoc#automated-rebalance-failure-handling[below].

Before attempting rebalance with a reduced number of nodes, assess whether the available resources can support the intended number of replicas.
See xref:learn:clusters-and-availability/removal.adoc[Removal], for guidance.

[#automated-rebalance-failure-handling]
=== Automated Rebalance-Failure Handling

The handling of a rebalance-failure can be configured to occur automatically.
Configuration occurs by means of the *General* settings screen.
Up to 3 _retries_ can be configured.
Each retry occurs after the elapsing of a time-period specified in seconds, by the administrator.
By default, automated rebalance-failure handling is _not_ enabled.
For detailed information, see xref:manage:manage-settings/general-settings.adoc[General Settings].

If automated rebalance-failure handling has been enabled (meaning that between 1 and 3 retries have been specified), following a rebalance failure, the following notifications appear at the lower left of the main screen of Couchbase Web Console:

image::manage-nodes/rebalanceFailureMessage.png[,260,align=middle]

The middle notification indicates that a retry is planned, in accordance with the configuration made on the *General* settings screen.
The displayed integer represents the number of seconds remaining before the retry is commenced; and is gradually decremented, from the configured maximum to zero; at which point, retry is commenced.

If a retry fails, additional retries occur successively; until the maximum configured number have been attempted.

[#retry-cancellation]
=== Retry-Cancellation

If one or more retries have been configured, and, following a rebalance failure, a retry is pending, no administrative tasks of any kind should be performed on the cluster.
Before performing any further administrative task, _either_ allow configured retries continue occurring &#8212; until one has succeeded, or all have failed; _or_ cancel the entire retry sequence.

To cancel, left-click on the *CANCEL RETRY* link, on the retry notification.
Note that this cancels *all* currently scheduled retries.
However, the configured number of retries will be rescheduled for each subsequent, manually initiated rebalance.

[#add-a-node-with-the-cli]
== Add a Node with the CLI

To add a new Couchbase Server-node to an existing cluster, use the xref:cli:cbcli/couchbase-cli-server-add.adoc[server-add] command.
Note that this command requires that arguments be provided for its `--server-add-username` and `--server-add-password` flags.
In this case, meaningful arguments do not exist, since the new node features an instance of Couchbase Server that is running, but has not been provisioned with a username or password.
Therefore, specify placeholder arguments. Additionally, specify that the `data` service be run on the node, once it is part of the cluster.

----
couchbase-cli server-add -c 10.142.181.101:8091 --username Administrator \
--password password --server-add 10.142.181.102:8091 \
--server-add-username someName --server-add-password somePassword \
--services data
----

If successful, the command returns the following:

----
SUCCESS: Server added
----

The newly added node must now be rebalanced into the cluster. Use the xref:cli:cbcli/couchbase-cli-rebalance.adoc[rebalance] command:

----
couchbase-cli rebalance -c 10.142.181.101:8091 \
--username Administrator \
--password password
----

During rebalance, progress is displayed as console output:

----
Rebalancing
Bucket: 01/01 (travel-sample)                      60714 docs remaining
[=====                                                          ] 4.56%
----

If successful, the command returns the following:

----
SUCCESS: Rebalance complete
----

Note that when the operation is highly complex, it may be desirable to get status on its progress, or stop the operation. See the command reference for xref:cli:cbcli/couchbase-cli-rebalance-status.adoc[rebalance-status] and xref:cli:cbcli/couchbase-cli-rebalance-stop.adoc[rebalance-stop], for more information.

[#add-a-node-with-the-rest-api]
== Add a Node with the REST API

To add a new Couchbase Server-node to an existing cluster, use the `/controller/addNode` URI.
The following command adds node `10.142.181.102` to cluster `10.142.181.101`:

----
curl -u Administrator:password -v -X POST \
10.142.181.101:8091/controller/addNode \
-d 'hostname=10.142.181.102&user=someName&password=somePassword&services=kv'
----

Note that the argument passed for `services` is `kv`, which signifies the Data Service.
Optionally, other services can be specified: `index` (Index Service), `n1ql` (Query Service), `eventing` (Eventing Service), `fts` (Search Service), and `cbas` (Analytics Service).
If multiple services are specified, this should be as a comma-separated list: for example, `n1ql,index,fts`.
As with the CLI command shown above, a username and password are expected, even though in this case, the new node has not been provisioned: therefore, placeholders are used. If successful, the command returns the name of the newly added node:

----
{"otpNode":"ns_1@10.142.181.102"}
----

The newly added node must now be rebalanced into the cluster. Use the `/controller/rebalance` URI, as follows:

----
curl -u Administrator:password -v -X POST \
10.142.181.101:8091/controller/rebalance \
-d 'knownNodes=ns_1@10.142.181.101,ns_1@10.142.181.102'
----

Note that the `knownNodes` argument lists each of the nodes in the cluster.
If successful, the command returns no output.

For further information on adding nodes with the REST API, see xref:rest-api:rest-cluster-addnodes.adoc[Adding Nodes to Clusters]; on rebalancing, see xref:rest-api:rest-cluster-rebalance.adoc[Rebalancing Nodes].


[#next-steps-after-adding-and-rebalancing]
== Next Steps

As well as supporting a cluster's adding a node to itself, Couchbase Server also supports a node's joining itself to a cluster (which is essentially the same operation, but proceeding from the node, rather than from the cluster).
See xref:manage:manage-nodes/join-cluster-and-rebalance.adoc[Join a Cluster and Rebalance] for details.
