= Perform Graceful Failover
:description: Graceful failover allows a node to be removed from a cluster proactively, when the cluster is healthy, and all data is available.
:page-aliases: clustersetup:setup-failover-graceful

[abstract]
{description}

[#understanding-graceful-failover]
== Understanding Graceful Failover

_Graceful_ failover allows a Data Service node to be removed from the cluster _proactively_, in an orderly and controlled fashion (say, for the purposes of system-maintenance).
It is manually initiated when the entire cluster is in a healthy state, and all active and replica vBuckets on all nodes are available.

A complete conceptual description of failover and its variants (including graceful) is provided in xref:learn:clusters-and-availability/failover.adoc[Failover].

[#examples-on-this-page-graceful-failover]
== Examples on This Page

The examples in the subsections below fail the same node over gracefully, from the same two-node cluster; using the xref:manage:manage-nodes/failover-graceful.adoc#graceful-failover-with-the-ui[UI], the xref:manage:manage-nodes/failover-graceful.adoc#graceful-failover-with-the-cli[CLI], and the xref:manage:manage-nodes/failover-graceful.adoc#graceful-failover-with-the-rest-api[REST API] respectively. The examples assume:

* A two-node cluster already exists; as at the conclusion of xref:manage:manage-nodes/join-cluster-and-rebalance.adoc[Join a Cluster and Rebalance].

* The cluster has the Full Administrator username of `Administrator`, and password of `password`.

[#graceful-failover-with-the-ui]
== Graceful Failover with the UI

Proceed as follows:

. Access the Couchbase Web Console *Servers* screen, on node `10.142.181.101`, by left-clicking on the *Servers* tab in the left-hand navigation bar.
The display is as follows:
+
[#servers-screen-with-node-added-after-rebalance]
image::manage-nodes/twoNodeClusterAfterRebalanceCompressedView.png[,800,align=middle]
+
. To see further details of each node, left-click on the row for the node.
The row expands vertically, as follows:
+
[#two-node-cluster-after-rebalance-expanded]
image::manage-nodes/twoNodeClusterAfterRebalance.png[,800,align=middle]

. To initiate failover, left-click on the *Failover* button, at the lower right of the row for `101.142.181.102`:
+
[#failover-button]
image::manage-nodes/failoverButton.png[,140,align=middle]
+
The *Confirm Failover Dialog* now appears:
+
[#confirm-failover-dialog]
image::manage-nodes/confirmFailoverDialog.png[,400,align=middle]
+
Two radio buttons are provided, to allow selection of either *Graceful* or *Hard* failover.
*Graceful* is selected by default.

. Confirm _graceful_ failover by left-clicking on the *Failover Node* button.
+
Graceful failover is now initiated, and a rebalance occurs as part of the procedure.
A progress dialog appears, summarizing overall progress:
+
[#graceful-failover-fullscreen-progress]
image::manage-nodes/rebalanceFollowingGracefulFailover6.5v2.png[,400,align=middle]
+
For server-level details of the graceful failover process, see the conceptual overview provided in xref:learn:clusters-and-availability/graceful-failover.adoc[Graceful Failover].
+
When the process ends, the display is as follows:
+
[#graceful-failover-fullscreen-rebalance-needed]
image::manage-nodes/gracefulFailoverFullScreenRebalanceNeeded.png[,800,align=middle]
+
This indicates the graceful failover has successfully completed, but an additional rebalance is required to complete the reduction of the cluster to one node.
+
. Left-click the *Rebalance* button, at the upper right, to initiate a further rebalance.
When the process is complete, the *Server* screen appears as follows:
+
[#graceful-failover-after-rebalance]
image::manage-nodes/gracefulFailoverAfterRebalance.png[,800,align=middle]
+
Node `10.142.181.102` has successfully been removed.

Note that if rebalance fails, notifications are duly provided.
These are described in xref:manage:manage-nodes/add-node-and-rebalance.adoc#rebalance-failure-notification[Rebalance Failure Notification].
See also the information provided on xref:manage:manage-nodes/add-node-and-rebalance.adoc#automated-rebalance-failure-handling[Automated Rebalance-Failure Handling], and the procedure for its set-up, described in xref:manage:manage-settings/general-settings.adoc#rebalance-settings[Rebalance Settings].

[#graceful-failover-with-the-cli]
== Graceful Failover with the CLI

To fail a node over gracefully, use the `failover` command, as follows:

----
couchbase-cli failover -c 10.142.181.101:8091 \
--username Administrator \
--password password \
--server-failover 10.142.181.102:8091
----

The `--server-failover` flag specifies the name and port number of the node to be gracefully failed over.

Progress is displayed as console output:

----
Gracefully failing over
Bucket: 00/00 ()                                 0 docs remaining
[======================                                   ] 17.77
----

When the progress completes successfully, the following output is displayed:

----
SUCCESS: Server failed over
----

The cluster can now be rebalanced with the following command, to remove the failed-over node:

----
couchbase-cli rebalance -c 10.142.181.101:8091 \
--username Administrator \
--password password \
--server-remove 10.142.181.102:8091
----

If successful, the operation gives the following output:

----
SUCCESS: Rebalance complete
----

For more information on `failover`, see xref:cli:cbcli/couchbase-cli-failover.adoc[failover]. For more information on `rebalance`, see xref:cli:cbcli/couchbase-cli-rebalance.adoc[rebalance].

[#graceful-failover-with-the-rest-api]
== Graceful Failover with the REST API

To fail a node over gracefully with the REST API, use the `/controller/startGracefulFailover` URI, specifying the node to be failed over, as follows:

----
curl -v -X POST -u Administrator:password \
http://10.142.181.101:8091/controller/startGracefulFailover \
-d 'otpNode=ns_1@10.142.181.102'
----

Note that the `optNode` flag can accept multiple values, separated by commas: every referenced node is gracefully failed over.

Subsequently, the cluster can be rebalanced, and the failed-over node removed, with the `/controller/rebalance` URI:

----
curl  -u Administrator:password -v -X POST \
http://10.142.181.101:8091/controller/rebalance \
-d 'knownNodes=ns_1@10.142.181.101,ns_1@10.142.181.102&ejectedNodes=ns_1@10.142.181.102'
----

For more information on `/controller/startGracefulFailover`, see xref:rest-api:rest-failover-graceful.adoc[Setting Graceful Failover].
For more information on `/controller/rebalance`, see xref:rest-api:rest-cluster-rebalance.adoc[Rebalancing the Cluster].

[#next-steps-after-graceful-failover]
== Next Steps
A _hard_ failover can be used when a node is unresponsive. See xref:manage:manage-nodes/failover-hard.adoc[Hard Failover].
