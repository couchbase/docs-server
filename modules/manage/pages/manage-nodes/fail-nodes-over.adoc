= Fail a Node Over and Rebalance
:description: pass:q[Nodes can be _failed over_, and thereby removed safely from a cluster in the event of unavoidable downtime, without any break in the serving of data to applications.]
:page-aliases: clustersetup:failover

[abstract]
{description}

[#understanding-failover]
== Understanding Failover

A complete conceptual description of failover is provided in xref:learn:clusters-and-availability/failover.adoc[Failover].
There are two basic types: _graceful_ and _hard_.

* _Graceful_ allows a Data Service node to be removed from the cluster _proactively_, in an orderly and controlled fashion (say, for the purposes of system-maintenance).
It is manually initiated when the entire cluster is in a healthy state, and all active and replica vBuckets on all nodes are available.

* _Hard_: The ability to drop a node from the cluster _reactively_, because the node has become unavailable or unstable. It is manually or automatically initiated, and should be applied after the point at which active vBuckets have been lost.
If applied to an available node running the Data Service, ongoing writes and replications may be interrupted.

The automatic initiation of hard failover is known as _automatic_ failover, and is configured by means of the xref:manage:manage-settings/general-settings.adoc[General] settings screen, in the *Settings* area of Couchbase Web Console; or by means of equivalent CLI and REST API commands.

Note that when a node is flagged for _failover_ (as opposed to _removal_), replica vBuckets are lost when rebalance occurs, following node-removal. By contrast, _removal_ creates new copies of replica vBuckets that would otherwise be lost (thereby creating greater competition for the memory resources of the remaining nodes).

This section shows how graceful and hard failover can be initiated.
