= Perform Hard Failover
:page-aliases: clustersetup:hard-failover

[abstract]
Hard failover allows a node to be removed from a cluster reactively, because the node has become unresponsive or unstable.

[#understanding-hard-failover]
== Understanding Hard Failover

_Hard_ failover drops a node from a cluster _reactively_, because the node has become unresponsive or unstable.
It is manually or automatically initiated, and occurs after the point at which active vBuckets have been lost.

The automatic initiation of hard failover is known as _automatic_ failover, and is configured by means of the xref:manage:manage-settings/general-settings.adoc#node-availability[Node Availability] screen, in the *Settings* area of Couchbase Web Console, or by means of equivalent CLI and REST API commands.
This page explains how to initiate hard failover manually.

A complete conceptual description of failover and its variants (including hard) is provided in xref:learn:clusters-and-availability/failover.adoc[Failover].

[#examples-on-this-page-hard-failover]
== Examples on This Page

The examples in the subsections below perform the same _hard_ failover, on the same two-node cluster; using the xref:manage:manage-nodes/failover-hard.adoc#hard-failover-with-the-ui[UI], the xref:manage:manage-nodes/failover-hard.adoc#hard-failover-with-the-cli[CLI], and the xref:manage:manage-nodes/failover-hard.adoc#hard-failover-with-the-rest-api[REST API] respectively. The examples assume:

* A two-node cluster already exists; as at the conclusion of xref:manage:manage-nodes/join-cluster-and-rebalance.adoc[Join a Cluster and Rebalance].

* The cluster has the Full Administrator username of `Administrator`, and password of `password`.

[#hard-failover-with-the-ui]
== Hard Failover with the UI

Proceed as follows:

. Access the Couchbase Web Console *Servers* screen, on node `10.142.181.101`, by left-clicking on the *Servers* tab in the left-hand navigation bar.
The display is as follows:
+
[#servers-screen-with-node-added-after-rebalance]
image::manage-nodes/twoNodeClusterAfterRebalanceCompressedView.png[,800,align=middle]
+
. To see further details of the node to be failed over, which in this example will be `101.142.181.102`, left-click on the row for the node.
The row expands vertically, as follows:
+
[#two-node-cluster-after-rebalance-expanded]
image::manage-nodes/twoNodeClusterAfterRebalance.png[,800,align=middle]

. To initiate failover, left-click on the *Failover* button, at the lower right of the row for `101.142.181.102`:
+
[#failover-button]
image::manage-nodes/failoverButton.png[,140,align=middle]
+
The *Confirm Failover Dialog* now appears:
+
[#confirm-failover-dialog]
image::manage-nodes/confirmFailoverDialog.png[,400,align=middle]
+
Two radio buttons are provided, to allow selection of either *Graceful* or *Hard* failover. *Graceful* is selected by default.

. Select _hard_ failover by selecting the *Hard* radio button:
+
[#select-hard-failover]
image::manage-nodes/confirmHardFailoverDialog.png[,400,align=middle]
+
Note the warning message that appears when hard failover is selected: in particular, this points out that hard failover may interrupt ongoing writes and replications, and that therefore it may be better to xref:manage:manage-nodes/remove-node-and-rebalance.adoc[Remove a Node and Rebalance], than use hard failover on a still-available Data Service node.
+
To continue with hard failover, confirm your choice by left-clicking on the *Failover Node* button.
+
Hard failover now occurs.
On conclusion, the *Servers* screen appears as follows:
+
[#cluster-after-hard-failover]
image::manage-nodes/twoNodeClusterAfterHardFailover.png[,800,align=middle]
+
This indicates that hard failover has successfully completed, but a rebalance is required to complete the reduction of the two-node cluster to one node.
+
. Left-click the *Rebalance* button, at the upper right, to initiate rebalance.
When the process is complete, the *Server* screen appears as follows:
+
[#graceful-failover-after-rebalance]
image::manage-nodes/gracefulFailoverAfterRebalance.png[,800,align=middle]
+
Node `10.142.181.102` has successfully been removed.

Note that if rebalance fails, notifications are duly provided.
These are described in xref:manage:manage-nodes/add-node-and-rebalance.adoc#rebalance-failure-notification[Rebalance Failure Notification].
See also the information provided on xref:manage:manage-nodes/add-node-and-rebalance.adoc#automated-rebalance-failure-handling[Automated Rebalance-Failure Handling], and the procedure for its set-up, described in xref:manage:manage-settings/general-settings.adoc#rebalance-settings[Rebalance Settings].

[#hard-failover-of-multiple-nodes]
=== Hard Failover of Multiple Nodes

Hard failover of one or more nodes can be managed by means of the *FAILOVER* tab, toward the upper right of the *Servers* screen:

image::manage-nodes/serverScreenWithFailoverTab.png[,800,align=middle]

As the *Servers* screen here shows, this example features a cluster of three nodes.
Left-click on the *FAILOVER* tab to perform hard failover on one or more of the three nodes:

image::manage-nodes/leftClickOnFailoverTab.png[,200,align=middle]

This brings up the *Failover Multiple Nodes* dialog:

image::manage-nodes/hardFailoverMultipleNodesDialog.png[,520,align=middle]

The dialog provides the following *Data Loss Warning*: _For hard failover of multiple nodes, each Couchbase bucket must have at least as many replicas as the total number of nodes failed over or you WILL lose data.
Since hard failover removes nodes immediately it may also result in failure of in-flight operations._

Select one or more nodes from the checkboxes, then left-click on the *Failover Nodes* button, to start hard failover.
When failover has complete, a rebalance will, as usual, be required.

[#hard-failover-with-the-cli]
== Hard Failover with the CLI

To perform hard failover on a node, use the `failover` command with the `--force` flag, as follows:

----
couchbase-cli failover -c 10.142.181.102:8091 \
--username Administrator \
--password password \
--server-failover 10.142.181.102:8091 --force
----

The `--force` flag specifies that failover be hard.

When the progress completes successfully, the following output is displayed:

----
SUCCESS: Server failed over
----

The cluster can now be rebalanced with the following command, to remove the failed-over node:

----
couchbase-cli rebalance -c 10.142.181.101:8091 \
--username Administrator \
--password password --server-remove 10.142.181.102:8091
----

Progress is displayed as console output. If successful, the operation gives the following output:

----
SUCCESS: Rebalance complete
----

For more information on `failover`, see
xref:cli:cbcli/couchbase-cli-failover.adoc[failover]. For
more information on `rebalance`, see
xref:cli:cbcli/couchbase-cli-rebalance.adoc[rebalance].

[#hard-failover-with-the-rest-api]
== Hard Failover with the REST API

To perform hard failover on a node with the REST API, use the `/controller/failover` URI, specifying the node to be failed over, as follows:

----
curl -v -X POST -u Administrator:password \
http://10.142.181.101:8091/controller/failOver \
-d 'otpNode=ns_1@10.142.181.102'
----

Subsequently, the cluster can be rebalanced, and the failed-over node removed, with the `/controller/rebalance` URI:

----
curl  -u Administrator:password -v -X POST \
http://10.142.181.101:8091/controller/rebalance \
-d 'ejectedNodes=ns_1%4010.142.181.102' \
-d 'knownNodes=ns_1%4010.142.181.101%2Cns_1%4010.142.181.102'
----

For more information on `/controller/failover`, see xref:rest-api:rest-node-failover.adoc[Failing Over Nodes].
For more information on `/controller/rebalance`, see xref:rest-api:rest-cluster-rebalance.adoc[Rebalancing Nodes].

[#next-steps-after-hard-failover]
== Next Steps
A node that has been failed over can be recovered and reintegrated into the cluster.
See xref:manage:manage-nodes/recover-nodes.adoc[Recover a Node].
