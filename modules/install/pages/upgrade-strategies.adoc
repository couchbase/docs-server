= Upgrade Options
:description: A Couchbase-Server cluster can be upgraded in different ways.
//:page-aliases: install:upgrade-strategy-for-features

[abstract]
{description}

[#understanding-upgrade]
== Understanding Upgrade

Couchbase Server can be upgraded in different ways; each having its own benefits and requirements.
This page presents an overview of the available options: all should be fully evaluated before a particular approach is selected, and cluster-upgrade performed.

[#upgrade-and-availability]
== Upgrade and Availability

Upgrade of Couchbase Server occurs node by node.
When every node has been upgraded to the latest version of Couchbase Server, the whole cluster is considered upgraded.

To be upgraded, each node must be _offline_: this means that although it is still _up_ and _network-accessible_, it is _not_ part of a cluster that is serving data.
Therefore, to upgrade all the nodes in the cluster, one of the following approaches must be selected:

* All the nodes of the cluster are taken offline together; and consequently, the cluster stops serving data.
Each of the nodes is upgraded.
Then, a rebalance is performed, the cluster is brought back online, and the serving of data is resumed.
+
This approach to cluster-upgrade (referred to as _cluster offline_) offers the greater simplicity.
However, it necessitates cluster-downtime.

* Each node in turn is taken offline; is upgraded; and is re-introduced into the cluster &#8212; by means of either _adding_ or _joining_, as described in xref:learn:clusters-and-availability/nodes.html#clusters[Clusters].
This means that while any given node is offline, all other nodes remain online.
+
This process (referred to as _cluster online_) is more complex, but allows the cluster to continue serving data without interruption.
Its greatest potential drawback is the overhead involved in repeatedly taking nodes out of the cluster, and then duly re-introducing them: given that a full xref:learn:clusters-and-availability/rebalance.adoc[Rebalance], whenever performed, is likely to consume considerable cluster-resources for a significant duration.
However, overhead can be minimized, by means of either _swap rebalance_ or _graceful failover_.

[#swap-rebalance]
=== Swap Rebalance

_Swap Rebalance_ is automatically performed by Couchbase Server when all the following conditions apply:

* One or more nodes have been introduced into the cluster.

* One or more nodes have been taken out of the cluster.

* The introduced nodes are identical in number and configuration to the nodes that have been taken out.

* Rebalance is triggered by the administrator.

Since the introduced nodes are recognized by Couchbase Server to have equivalent capacities and configurations to those that have been taken out, rebalance is performed as a _swap rebalance_; which confines its activity to the incoming and outgoing nodes.
Thus, for example, if one Data Service node is removed and another added, the swap rebalance simply transfers the vBucket layout of the outgoing node directly onto the incoming node; with the layouts of other Data Service nodes not requiring modification.

By contrast, if two Data Service nodes are taken out, and one Data Service node and one Search Service node are introduced, since the incoming and outgoing nodes differ in configuration, when rebalance is triggered by the administrator, Couchbase Server performs a _full_ rebalance; involving more nodes than those in transit; and indeed, potentially involving the entire cluster.

Note that the effect of rebalance on different Couchbase Services is described in xref:learn:clusters-and-availability/rebalance.adoc[Rebalance]: familiarity with this information is required before proceeding.

[#using-spare-nodes]
==== Using Spare Nodes

Since swap rebalance requires the introduction and removal of nodes that are identical in number and configuration, _spare_ nodes must be used.
A _spare_ node is a node that has not previously been included in the cluster, and may not be intended for long term future inclusion.

For example, if the upgrade procedure is intended to upgrade one node at a time, a spare node must be introduced into the cluster in order to allow one of the existing nodes to be removed: swap rebalance is thereby confined to those two nodes; and when it is complete, the introduced spare node takes over the role of the node to be upgraded.
When the upgraded node is added (or joined) back into the cluster, the spare node is in turn removed, and swap rebalance again occurs for those two nodes; this time copying vBuckets and/or other forms of data back onto the original node.

If additional machines are indeed available to be used as _spare nodes_, these will allow the cluster to continue to serve data at full capacity.
This is described below, in xref:install:upgrade-strategies.adoc#cluster-online-swap-rebalance-at-full-capacity[Cluster Online: Swap Rebalance at Full Capacity].

If no additional machines are available to be used as _spare nodes_, it must be determined whether acceptable cluster-performance can be maintained throughout the entire cluster-upgrade procedure, if one or more nodes are removed for the duration, and the cluster thus left to serve data at reduced capacity.
If acceptable performance can indeed be maintained under such conditions, the appropriate number of nodes should be removed from the cluster, and a full rebalance performed.
From this point, the diminished cluster will continue to serve data at acceptable performance-levels; while the nodes that have been removed are used as spares throughout the upgrade.
This is described below, in xref:install:upgrade-strategies.adoc#cluster-online-swap-rebalance-with-capacity-reduced[Cluster Online: Swap Rebalance with Capacity Reduced].

[#using-graceful-failover]
==== Using Graceful Failover

If it is possible to remove one or more Data Service nodes from the cluster, and run the cluster at reduced capacity for the duration of the upgrade procedure without performance-levels becoming unacceptable, xref:manage:manage-nodes/failover-graceful.adoc[Graceful Failover] can be used to bring individual Data Service nodes out of the cluster, and so allow them to be upgraded.
The Graceful Failover procedure ensures that all the cluster's active vBuckets continue to be available, on the remaining Data Service nodes, after the node to be upgraded has been failed over.
Subsequently, the upgraded node is restored to the cluster using xref:learn:clusters-and-availability/recovery.html#delta-recovery[Delta Recovery].

The main advantage of Graceful Failover and Delta Recovery in this context is high performance and simplicity of management.
The main constraint of this option is that it can _only_ be used for nodes running the Data Service alone.

Further considerations are provided in xref:learn:clusters-and-availability/graceful-failover.html#advantages-and-disadvantages[Advantages and Disadvantages].
(Note also that this option is not available when choosing to upgrade with _net-new_ systems &#8212; as in many Cloud-based deployments; since the new Data Service nodes do not have previous nodesâ€™ data in place.)

[#online-upgrade]
== Upgrade with Cluster Online

An _online upgrade_ means that the cluster continues to serve data while its nodes are progressively upgraded.
Online upgrade can be performed in any of the following ways.

[#cluster-online-swap-rebalance-with-capacity-reduced]
=== Cluster Online: Swap Rebalance at Full Capacity

One or more spare nodes, which exist in addition to those committed to the cluster, are prepared for addition to the cluster.
When these nodes are added to the cluster, the same number are removed.
Addition occurs by means of either _joining_ or _adding_, as described in xref:learn:clusters-and-availability/nodes.html#clusters[Clusters].
Note that the configuration of the added nodes must match that of the removed nodes.
When rebalance is triggered by the administrator, Couchbase Server performs a _swap rebalance_.

Removed nodes are kept _up_ and _network-accessible_: and in this state, are upgraded to the latest version of Couchbase Server.
Then, following the upgrade procedure, the upgraded nodes are re-introduced into the cluster; and are given configuration that match the configurations of the spare nodes; and the spare nodes are themselves now removed.
Finally, a further xref:learn:clusters-and-availability/rebalance.adoc[Rebalance] is performed, and the upgraded nodes become full members of the cluster.

Once all nodes have been processed in this way, the entire cluster has been upgraded.

[#cluster-online-swap-rebalance-with-capacity-reduced]
=== Cluster Online: Swap Rebalance with Capacity Reduced

An assessment is made of how many nodes can be removed from the cluster while maintaining acceptable data-serving performance.
A number of nodes no greater than the ascertained number is then removed, and a rebalance performed.
The diminished cluster continues to serve data.

Upgrade now commences.
One or more nodes are added to the cluster, and the same number are removed.
The added nodes are configured such that when rebalance is triggered by the administrator, Couchbase Server performs a _swap rebalance_.
Removed nodes are kept _up_ and _network-accessible_: and in this state, are upgraded to the latest version of Couchbase Server.
Then, following the upgrade procedure, the upgraded nodes are re-introduced into the cluster: each can either be _joined_ or _added_, as described in xref:learn:clusters-and-availability/nodes.html#clusters[Clusters].
The configuration of added nodes must match that of the spare nodes that are now removed.
Finally, a further xref:learn:clusters-and-availability/rebalance.adoc[Rebalance] is performed, and the upgraded nodes become full members of the cluster.

Once all nodes have been processed in this way, the entire cluster has been upgraded.

[#feature-availability]
=== Feature Availability During Upgrade

Certain features of Couchbase Server may not be available while the upgrade of an online cluster is in progress; since the cluster is during this period running two different versions of Couchbase Server, and the features of the later version are not available to nodes still running the earlier.

For details, see xref:install:upgrade-strategy-for-features.adoc[Upgrading to Couchbase Server 7.0].

[#offline-upgrade]
== Upgrade with Cluster Offline

When the entire cluster is offline, each node can be individually upgraded, before a concluding full rebalance is required.
Before upgrade of any node is performed, xref:learn:clusters-and-availability/automatic-failover.adoc[Automatic Failover] should be _disabled_; and should be _re-enabled_ only when upgrade is complete.
