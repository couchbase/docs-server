= Setting Alternate Addresses
:page-topic-type: reference

[abstract]
Alternate addresses and port-numbers can be established for and removed from nodes, by means of the `PUT` and `DELETE` HTTP methods, using the `/node/controller/setupAlternateAddresses/external` URI.

[#http-method-and-uri]
== HTTP methods and URI

----
PUT /node/controller/setupAlternateAddresses/external

DELETE /node/controller/setupAlternateAddresses/external
----

[#rest-setup-alternate-address-description]
== Description

An alternate address can be established for and removed from a node; as can alternate port-numbers, for services running on the node.
This permits node-access by external applications unable to handle cluster-private or otherwise non-standard node-naming conventions &#8212; which are sometimes required in cloud-based deployments.

[#curl-syntax]
== Curl Syntax

----
curl -v -X PUT -u [admin]:[password]
  http://[localhost]:8091/node/controller/setupAlternateAddresses/external
  [-d hostname=<alternate-address> ]
  [-d <service-name>=<alternate-port-number> ]+

curl -v -X DELETE -u [admin]:[password]
  http://[localhost]:8091/node/controller/setupAlternateAddresses/external
----

The parameters are:

* `localhost`.
The address of the cluster-node to which an alternate address is to be assigned.

* `hostname`.
The alternate address to be assigned to the cluster-node.
This can be either a _hostname_ or an _IP address_ (either V4 or V6).

* `service`.
Optionally, one or more service-names corresponding to services running on the node, each service-name being specified with an alternate port number.
Possible service-names are `kv` (Data Service), `index` (Index Service), `n1ql` (Query Service), `fts` (Search Service), `cbas` (Analytics Service), and `eventing` (Eventing Service).

Each successive use of the `PUT` method entirely deletes all previous alternate settings (hostname and service) on the node.
The `DELETE` method deletes all alternate settings (hostname and service).

[#responses]
== Responses

For both methods, success gives the status `200 OK`, with no object returned.

If no hostname is specified, the message `hostname should be specified` is displayed.

If either method is used to specify an unknown node, the message `No route to host` is given, and the call fails.

Specifying an invalid service-name gives the status `400 Bad Request`, with the message `Invalid Port "<submitted-service-name>" : No such port.`


[#examples]
== Examples

The following examples demonstrate how to specify alternate addresses and port-numbers, and how to remove them.

[#assign-alternate-address-and-port-numbers]
=== Assign an Alternate Address and Port-Numbers

To assign and alternate address and alternate port-numbers, use the `PUT /node/controller/setupAlternateAddresses/external` method and URI.

----
curl -v -X PUT -u Administrator:password \
http://10.143.192.101:8091/node/controller/setupAlternateAddresses/external \
-d hostname=10.10.10.11 \
-d kv=9000 \
-d n1ql=9050
----

This assigns the alternate address `10.10.10.11` to node 10.143.192.101, and the alternate port-numbers `9000` and `9050` to the Data and Query Services, respectively.
No object is returned.

[#list-alternate-addresses-and-port-numbers]
=== List Alternate Addresses and Port Numbers

To list alternate addresses and port numbers, use the `GET /pools/default/nodeServices` method and URI.
In the following example, the output is piped to the `jq` tool, to provide optimal readability.

----
curl -v -X GET -u Administrator:password \
http://10.143.192.101:8091/pools/default/nodeServices | jq

----

The output is as follows:

----

----





To determine which nodes are currently in the cluster, use the `PUT /node/controller/setupAlternateAddresses/external` HTTP method and URI.

----
curl  -u Administrator:password -X GET \
http://10.143.190.101:8091/pools/default | jq '.' | grep hostname
----

In this example, the (extensive) output is piped to the `jq` tool to be formatted, and the lines featuring the `hostname` are then filtered by `grep`, to ensure readability.

----
curl  -u Administrator:password -X GET \
http://10.143.190.101:8091/pools/default | jq '.' | grep hostname
----

The output contains the following lines, which specify all nodes currently in the cluster:

----
"hostname": "10.143.190.101:8091",
"hostname": "10.143.190.102:8091",
"hostname": "10.143.190.103:8091",
----

For more information on this method and URI, see xref:rest-api:rest-cluster-details.adoc[Viewing Cluster Details].

[#perform-rebalance-following-a-hard-failover]
=== Perform Rebalance Following a Hard Failover

The _hard failover_ of a node results in that node continuing to be a member of the cluster, but being unable to serve data.
Since its active vBuckets are unavailable, corresponding replica vBuckets are promoted to active status on the still-available nodes.
This means that data continues to be served, but is in an _imbalanced state_ across the cluster.
This requires rebalance.

For detailed conceptual information, see xref:learn:clusters-and-availability/hard-failover.adoc[Hard Failover].
For information on performing a hard failover, see xref:rest-api:rest-node-failover.adoc[Failing over Nodes].

To perform the rebalance, specify all cluster-nodes in the value for the `knownNodes` parameter, including the one or ones to which hard failover is known to have been applied:

----
curl  -u Administrator:password -v -X POST \
http://10.143.190.101:8091/controller/rebalance \
-d 'knownNodes=ns_1%4010.143.190.101%2Cns_1%4010.143.190.102%2Cns_1%4010.143.190.103'
----

On success, the status `200 OK` is given, and no object is returned.
The failed-over node has been removed, and the data is now distributed evenly across the surviving nodes.
The successful node-removal is confirmed by again examining the current cluster nodes:

----
curl  -u Administrator:password -X GET \
http://10.143.190.101:8091/pools/default | jq '.' | grep hostname
----

The output indicates which nodes are still in the cluster:

----
"hostname": "10.143.190.101:8091",
"hostname": "10.143.190.102:8091",
----

This confirms that `10.143.190.103` has been removed.

[#add-a-node-to-a-cluster-then-rebalance]
=== Add a Node to a Cluster, then Rebalance

Adding a node to a cluster is a two-step process.

. The `POST /controller/addNode` HTTP method and URI are used to add the node.
This allows service-deployment for the node to be specified.
A placeholder username and password can be specified, when adding an unprovisioned node.

. The `POST /controller/rebalance` HTTP method and URI are used to rebalance the added node into the cluster.
Include the new node in the `knownNodes` node-list.

For example, the following command adds node `10.143.1990.103` to the cluster from which it was removed, and assigns it the Data Service:

----
curl -u Administrator:password -X POST \
10.142.181.101:8091/controller/addNode \
-d 'hostname=10.143.190.103&user=someName&password=somePassword&services=kv'
----

If successful, this returns the following object, indicating that the node is now recognized as a member of the cluster:

----
{"otpNode":"ns_1@10.143.190.103"}
----

For more information on this method and URI, see xref:rest-api:rest-cluster-addnodes.adoc[Adding Nodes to Clusters].

Next, the added node is rebalanced into the cluster.
This allows it to take its share of the data-distribution.

----
curl  -u Administrator:password -v -X POST \
http://10.143.190.101:8091/controller/rebalance \
-d 'knownNodes=ns_1%4010.143.190.101%2Cns_1%4010.143.190.102%2Cns_1%4010.143.190.103'
----

On success, the response code `200 OK` is given, and no object is returned.
The cluster is now rebalanced.
At the conclusion, the cluster can again be checked for its current membership:

----
curl  -u Administrator:password -X GET \
http://10.143.190.101:8091/pools/default | jq '.' | grep hostname
----

The output now includes the following:

----
"hostname": "10.143.190.101:8091",
"hostname": "10.143.190.102:8091",
"hostname": "10.143.190.103:8091",
----

This confirms that `10.143.190.103` has been rebalanced into the cluster.

[#eject-a-node]
=== Eject a Node

To eject a node, use the `POST /controller/rebalance` HTTP method and URI.
Specify the entire current node-list for the cluster as the value of the `knownNodes` parameter.
Specify the list of nodes to be ejected as the value of the `ejectedNodes` parameter.

For example, the following command ejects node `10.143.190.103` from the cluster:

----
curl  -u Administrator:password -v -X POST \
http://10.143.190.101:8091/controller/rebalance \
-d 'ejectedNodes=ns_1%4010.143.190.103' \
-d 'knownNodes=ns_1%4010.143.190.101%2Cns_1%4010.143.190.102%2Cns_1%4010.143.190.103'
----

On success, the response code `200 OK` is given, and no object is returned.
At the conclusion, the cluster can again be checked for its current membership:

----
curl  -u Administrator:password -X GET \
http://10.143.190.101:8091/pools/default | jq '.' | grep hostname
----

The output now includes the following:

----
"hostname": "10.143.190.101:8091",
"hostname": "10.143.190.102:8091",
----

[#rest-cluster-rebalance-adjustduringcompaction]
== Adjusting Rebalance During Compaction

=== Description

If a rebalance is performed while a node is undergoing index compaction, rebalance delays may be experienced.
The parameter, `rebalanceMovesBeforeCompaction`, is used to improve rebalance performance: potentially, this results in a larger index.
This setting can be modified with the `POST /internalSettings` endpoint.
By default, it is 64.
This specifies that 64 vBuckets are to be moved per node; at which point all vBucket movement is paused, and index compaction is triggered.
Since index compaction is therefore not performed while vBuckets are being moved, a large `rebalanceMovesBeforeCompaction` value results in the server spending less time compacting indexes; potentially resulting in larger index files, which take up more disk space.

For example:

----
curl -X POST -u Administrator:password 'http://10.5.2.54:8091/internalSettings' \
    -d 'rebalanceMovesBeforeCompaction=256'
----

[#see-also]
== See Also

For conceptual information on rebalance, see xref:learn:clusters-and-availability/rebalance.adoc[Rebalance].
For information on how to retrieve status on an ongoing rebalance, see xref:rest-api:rest-get-rebalance-progress.adoc[Getting Rebalance Progress] and xref:rest-api:rest-get-cluster-tasks.adoc[Getting Cluster Tasks].

For conceptual information on hard failover, see xref:learn:clusters-and-availability/hard-failover.adoc[Hard Failover].
For information on performing a hard failover with the REST API, see xref:rest-api:rest-node-failover.adoc[Failing over Nodes].
For information on retrieving details of a cluster, including its current nodes, see xref:rest-api:rest-cluster-details.adoc[Viewing Cluster Details].
