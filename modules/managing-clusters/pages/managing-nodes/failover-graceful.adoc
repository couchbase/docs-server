= Graceful Failover

[abstract]
Graceful failover allows a node to be removed from a cluster proactively,
when the cluster is healthy, and all data is available.

[#understanding-graceful-failover]
== Understanding Graceful Failover

_Graceful_ failover allows a Data Service node to be removed from the cluster
_proactively_, in an orderly and controlled fashion (say, for the
purposes of system-maintenance). It is manually initiated when the
entire cluster is in a healthy state, and all active and replica
vBuckets on all nodes are available.

A complete conceptual description of failover and its variants (including
graceful) is provided in
xref:understanding-couchbase:clusters-and-availability/failover.adoc[Failover].

[#examples-on-this-page-graceful-failover]
== Examples on This Page

The examples in the subsections below show fail the same node over
gracefully, from the same two-node cluster; using the
xref:managing-clusters:managing-nodes/failover-graceful.adoc#graceful-failover-with-the-ui[UI],
the
xref:managing-clusters:managing-nodes/failover-graceful.adoc#graceful-failover-with-the-cli[CLI],
and the
xref:managing-clusters:managing-nodes/failover-graceful.adoc#graceful-failover-with-the-rest-api[REST
API] respectively. The examples assume:

The examples assume:

* A two-node cluster already exists; as at the conclusion of
xref:managing-clusters:managing-nodes/join-cluster-and-rebalance.adoc[Join a
Cluster and Rebalance].

* The cluster has the Full Administrator username of
`Administrator`, and password of `password`.

[#graceful-failover-with-the-ui]
== Graceful Failover with the UI

Proceed as follows:

. Access the Couchbase Web Console *Servers* screen, on
node `10.142.181.101`, by left-clicking on the *Servers* tab in the left-hand
navigation bar. The display is as follows:
+
[#servers-screen-with-node-added-after-rebalance]
image::managing-nodes/twoNodeClusterAfterRebalanceCompressedView.png[,800,align=middle]
+
. To see further details of each node, left-click on the row for
the node. The row expands vertically, as follows:
+
[#two-node-cluster-after-rebalance-expanded]
image::managing-nodes/twoNodeClusterAfterRebalance.png[,800,align=middle]

. To initiate failover, left-click on the *Failover* button, at the lower
right of the row for `101.142.181.102`:
+
[#failover-button]
image::managing-nodes/failoverButton.png[,140,align=middle]
+
The *Confirm Failover Dialog* now appears:
+
[#confirm-failover-dialog]
image::managing-nodes/confirmFailoverDialog.png[,400,align=middle]
+
Two radio buttons are provided, to allow selection of either *Graceful* or
*Hard* failover. *Graceful* is selected by default.

. Confirm _graceful_ failover by
left-clicking on the *Failover Node* button.
+
Graceful failover is now initiated. A progress dialog appears new the top
of the screen, summarizing overall progress; while each node-row also
features its own progress bar, indicating progress per node:
+
[#graceful-failover-fullscreen-progress]
image::managing-nodes/gracefulFailoverFullScreenProgress.png[,800,align=middle]
+
For server-level details of the graceful failover process, see the conceptual
overview provided in
xref:understanding-couchbase:clusters-and-availability/graceful-failover.adoc[Graceful
Failover].
+
When the process ends, the display is as follows:
+
[#graceful-failover-fullscreen-rebalance-needed]
image::managing-nodes/gracefulFailoverFullScreenRebalanceNeeded.png[,800,align=middle]
+
This indicates the graceful failover has successfully completed, but a rebalance
is required to complete the reduction of the cluster to one node.
+
. Left-click the *Rebalance* button, at the upper right, to initiate rebalance.
When the process is complete, the *Server* screen appears as follows:
+
[#graceful-failover-after-rebalance]
image::managing-nodes/gracefulFailoverAfterRebalance.png[,800,align=middle]
+
Node `10.142.181.102` has successfully been removed.

[#graceful-failover-with-the-cli]
== Graceful Failover with the CLI

To fail a node over gracefully, use the `failover` command, as follows:

----
couchbase-cli failover -c 10.142.181.101:8091 \
--username Administrator \
--password password \
--server-failover 10.142.181.102:8091
----

The `--server-failover` flag specifies the name and port number of the
node to be gracefully failed over.

Progress is displayed as console output:

----
Gracefully failing over
Bucket: 00/00 ()                                 0 docs remaining
[======================                                   ] 17.77
----

When the progress completes successfully, the following output is displayed:

----
SUCCESS: Server failed over
----

The cluster can now be rebalanced with the following command, to remove
the failed over node:

----
couchbase-cli rebalance -c 10.142.181.101:8091 \
--username Administrator \
--password password \
--server-remove 10.142.181.102:8091
----

If successful, the operation gives the following output:

----
SUCCESS: Rebalance complete
----

For more information on `failover`, see
xref:cli:cbcli/couchbase-cli-failover.adoc[failover]. For
more information on `rebalance`, see
xref:cli:cbcli/couchbase-cli-rebalance.adoc[rebalance].

[#graceful-failover-with-the-rest-api]
== Graceful Failover with the REST API

To fail a node over gracefully with the REST API, use the
`/controller/startGracefulFailover` URI, specifying the node
to be failed over, as follows:

----
curl -v -X POST -u Administrator:password \
http://10.142.181.101:8091/controller/startGracefulFailover \
-d 'otpNode=ns_1@10.142.181.102'
----

Subsequently, the cluster can be rebalanced, and the failed over
node removed, with the `/controller/rebalance` URI:

----
curl  -u Administrator:password -v -X POST \
http://10.142.181.101:8091/controller/rebalance \
-d 'knownNodes=ns_1@10.142.181.101,ns_1@10.142.181.102&ejectedNodes=ns_1@10.142.181.102'
----

For more information on `/controller/startGracefulFailover`, see
xref:rest-api:rest-failover-graceful.adoc[Setting Graceful Failover].
For more information on `/controller/rebalance`, see
xref:rest-api:rest-cluster-rebalance.adoc[Rebalancing Nodes].

[#next-steps-after-graceful-failover]
== Next Steps
A _hard_ failover can be used when a node is unresponsive. See
xref:managing-clusters:managing-nodes/failover-hard.adoc[Hard Failover].
