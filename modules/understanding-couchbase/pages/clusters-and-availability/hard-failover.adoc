= Hard Failover

[abstract]
Hard failover is the process of immediately removing an
unavailable or unstable node from
a cluster.

[#understanding-hard-failover]

== Understanding Hard Failover

Hard failover is the process of immediately
removing a node from a cluster, when that node is unavailable or unstable.
Hard failover tells the Cluster Manager to remove the
node from active use, regardless of what is occurring; and then to start the
procedures necessary for the other nodes in the cluster to take over
from the removed node.
The Cluster Manager performs slightly different actions for each service
running on the node being failed over.

Hard failover can be performed on any node in the cluster. However, it is
for handling emergencies, and should not be
used for scheduled or planned maintenance.

Hard failover can be initiated by means of Couchbase Web Console, the CLI,
or the REST API. It can also be initiated by means of _automatic_
failover.

== Hard Failover on Data Nodes

If the failed-over node is running the Data Service, the cluster initiates
multiple operations to make an active set of vBuckets available as soon as
possible.
Perform a hard failover _only_ when there is a full compliment of replica
vBuckets available to promote for each Bucket.
For example, if you have four nodes running the Data Service, and one node has
failed, there are 256 active and 256 replica vBuckets missing from the cluster.

The vBucket resources appear as follows:

image::clusters-and-availability/vbucket-resources.png[,350,align=left]

When hard failover is initiated, the Cluster Manager (which maintains
information on the location of all active and replica vBuckets on the
cluster) directs the remaining nodes to promote replica vBuckets to
active status.
It then communicates a revised cluster map to client applications and clusters.
The entire process typically requires a few milliseconds.

When the process is complete, no vBuckets remain on the failed-over
node, but the node is still not fully removed from the cluster.
The cluster is at this point considered to be in a _degraded state_, in terms of
availability; as is now contains a reduced number of replica vBuckets, and
so lacks resources to handle a further node-outage.

== Hard Failover on Index Nodes

When Global Secondary Indexes (GSI) are defined, each is by default
created on only one
node running the Index Service.
If that node fails, those indexes and their definitions are unavailable.
Should the node be repaired and added back via Delta Node Recovery, the
indexes will be updated and become available again.
If the node is replaced, the indexes need to be created again.
To rebalance a new node into the cluster, you must first remove the failed node,
using hard failover.

It is recommended to run the Index Services on at least two nodes, so that
the service remains available if one of the nodes goes down.

== Hard Failover on Query Nodes

Query Service nodes are stateless, and can be added and removed from the
cluster with no consequence to data.
However, any ongoing queries on those nodes will result in errors.
As long as the node to be removed is not the last Query Service node, the
cluster will continue to function, although possibly with reduced performance
on querying.


== Hard Failover and Search Nodes

Full Text Indexes are partitioned and automatically distributed across nodes,
if multiple nodes are running the Search Service.
When a node running the Search service is failed over, it stops taking traffic.
If there are no other nodes running Search, all index-building stops, and
searches fail.
If there is at least one other node running Search, these continue
responding to queries and return partial results.

When the administrator rebalances the cluster, it performs multiple operations,
depending on the level of redundancy you have designed for the Search Service
by configuring the replicas for the Full Text Index:

* If configured, the Search Service promotes the replicas to active on the
remaining nodes of the cluster.
* If not configured, the Search Service rebuilds the indexes on the remaining
nodes of the cluster, using stored index definitions.

The Search Service does not perform Delta Node Recovery.

== Returning the Cluster to a Stable State

If or when the failed node is repaired and ready, it can be added back to the
cluster via Delta Node Recovery or Full Recovery. Alternatively, an entirely
new node can be added instead.

* If Delta Node Recovery is an option, the Cluster Manager recognizes this
node as a previous member of the cluster.
If using the Couchbase Web Console, it will prompt the administrator to perform
the Delta Node Recovery.
In CLI, it will either perform the operation successfully; or will fail and
inform that you have to perform a full recovery.
+
When a node is added back to the cluster using Delta Node Recovery, the
replica vBuckets on the failed-over node are considered trusted but behind on
data.
The Cluster Manager will coordinate the vBuckets to become resynchronized,
which catches up the vBuckets on the node from where they left off to be current.
When the Cluster Manager has finished the synchronization, the vBucket
is promoted back to active status, and the cluster map is updated, since
this is a topology change.

* If the node is added back using Full Recovery, it is treated as an
entirely new node added to the cluster: it will be reloaded with data, and
needs a rebalance.
* The other option is to add a node and rebalance the cluster.

If you can, always attempt on returning the cluster to a properly sized
topology before rebalancing.
If you do a rebalance before adding the node back in, you can no longer
perform the Delta Node Rebalancing.

== A Hypothetical Scenario

Imagine you have a Couchbase bucket distributed across four nodes of the Data service in a cluster, where a node needs to be removed right this moment.
The server operations on-call engineer calls you at 11 PM on a Friday night to say that node #4 in the cluster is down.
The ops team has been unable to get the server back up for the last 10 minutes.

You have followed best practices and have the auto-failover configured.
For the Data service, with a four node cluster and one replica for each bucket, there are 256 active and 256 replica vBuckets on each of the four nodes, totaling 1024 active and 1024 replica vBuckets.
This particular example will only talk about one specific vBucket, #762, but this process is repeated for all the vBuckets on the node to be failed over:

. A hard failover is initiated (automatically or manually) to remove the node where the active vBucket 762 resides, node #4 in this example.
. The Cluster Manager promotes the replica vBucket 762 to active status on node #2.
+
After the vBucket promotion to active status, the cluster has no replica for vBucket 762 until a rebalance or the Delta Node Recovery, unless there are more replicas configured for this bucket.

. As this is a cluster topology change, the cluster map is updated so subsequent reads and writes by the Couchbase client SDKs will go to the correct location for data in vBucket 762, now node #2.

This process all happens in fractions of a second.
It is then repeated for the remaining 255 vBuckets of the bucket, one bucket at a time.
If there were more buckets, it would proceed to the next bucket and repeat the process there until complete.

What is happening in the application during this process, one might ask?
Until the down node is failed over (either automatically or manually) to promote the replica vBuckets to active, the application is receiving errors or timeouts for one-quarter of the reads and writes that would have gone to the now down node.
We had four nodes; now we have three.
If there were ten nodes in the Data service, the application would be unable to address one tenth of the data until failover is initiated.
If the application needs to read before failover happens, the application developer may want to use Replica Reads (see SDK-specific documentation), which is only used for such circumstances.

== Hard Failover versus Graceful?

Hard failover is a reactive action for an unhealthy node in the cluster.
Graceful failover is meant for planned maintenance.
Use hard failover when an unhealthy node needs to be ejected from the cluster right away and get back to 100% of the data available as soon as possible.

Hard failover and multiple nodes::
You should failover multiple nodes only at a time when there are enough replicas across all buckets of the Data service, and there are enough servers left so that the cluster can continue to operate.
+
Normally you would be able to failover one node per replica configured in the bucket/cluster.
For example, if you require the ability to failover two nodes, you must configure two replicas for each bucket.
Failure to do so will result in a loss of data.
Simply put, do not failover more nodes than there are replicas configured for all buckets.
+
The exception to the above rule is when xref:understanding-couchbase:clusters-and-availability/groups.adoc[Server Group Awareness] is enabled.
Server Group Awareness allows you to specify which server nodes are in a server rack in a data center, on different VM hosts, or different availability zones in the cloud.
It ensures that the replica vBuckets for the nodes residing in Rack A are never stored in Rack A.
When using Server Group Awareness, it is safe to failover an entire rackâ€™s worth of Couchbase nodes without data loss or interrupting your application, because the other racks contain the nodes with the replicas.

Hard failover when the cluster has not recognized that the node is down::
In rare cases, the Cluster Manager might fail to recognize that an unhealthy node is down.
If this occurs and a graceful failover is not successful, a hard failover can be the answer.
To initiate a hard failover for a node in this state, select the btn:[Fail Over] button using the Couchbase Web Console or use CLI.
+
If the nodeâ€™s health issue can be resolved, the node might be added back to the cluster.
A delta recovery will be presented as an option if the Cluster Manager detects that it is possible.
Otherwise, a full recovery must be used.
If the issue cannot be resolved, a replacement node should be added, and then the cluster rebalanced.
It is important to restore the cluster to a properly sized topology always before rebalancing.
Otherwise, you might cause additional failures as nodes become overloaded.
